{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageDeraining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1frXGdUERrMswNBwodIIloJErST5JHh_d",
      "authorship_tag": "ABX9TyOCcx38g/UmPWDxnOjTsvLH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pallabjyotidk/ImageDerainingProject/blob/main/ImageDeraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKvUIKCzhBLw",
        "outputId": "e78073de-5779-4ea5-fea0-e0eb3581ff51"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUD8awmcvZ6O",
        "outputId": "86550c61-b017-4be7-b17f-9a408b6fd876"
      },
      "source": [
        "cd drive/MyDrive/Project/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXtjR0Nj4lU8",
        "outputId": "6745842d-6643-4cdb-c1ee-64bf886cacde"
      },
      "source": [
        "pip install scipy==1.2.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scipy==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/39/066ecde98f373430bf7a39a02d91c7075b01ef4fc928456e8e31577342d6/scipy-1.2.0-cp37-cp37m-manylinux1_x86_64.whl (26.6MB)\n",
            "\u001b[K     |████████████████████████████████| 26.6MB 113kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.2.0) (1.19.5)\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy\n",
            "  Found existing installation: scipy 1.1.0\n",
            "    Uninstalling scipy-1.1.0:\n",
            "      Successfully uninstalled scipy-1.1.0\n",
            "Successfully installed scipy-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVnonAXRvM-z",
        "outputId": "2426b43c-4cd6-4322-c4d1-c7052562d20d"
      },
      "source": [
        "!python ID-CGAN.py"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-08 06:35:04.341027: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-04-08 06:35:05.998264: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-04-08 06:35:05.999217: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-04-08 06:35:06.027949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 06:35:06.028758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2021-04-08 06:35:06.028819: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-04-08 06:35:06.031985: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-04-08 06:35:06.032073: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-04-08 06:35:06.034080: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-04-08 06:35:06.034496: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-04-08 06:35:06.036538: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-04-08 06:35:06.037412: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-04-08 06:35:06.037636: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-04-08 06:35:06.037760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 06:35:06.038558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 06:35:06.039275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-04-08 06:35:06.039775: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-04-08 06:35:06.039922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 06:35:06.040663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2021-04-08 06:35:06.040722: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-04-08 06:35:06.040769: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-04-08 06:35:06.040823: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-04-08 06:35:06.040882: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-04-08 06:35:06.040927: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-04-08 06:35:06.040975: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-04-08 06:35:06.041021: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-04-08 06:35:06.041068: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-04-08 06:35:06.041161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 06:35:06.042002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 06:35:06.042714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-04-08 06:35:06.042786: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-04-08 06:35:06.508140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-04-08 06:35:06.508202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
            "2021-04-08 06:35:06.508226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
            "2021-04-08 06:35:06.508485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 06:35:06.509303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 06:35:06.510128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 06:35:06.510854: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-04-08 06:35:06.510915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10637 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3504: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
            "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n",
            "2021-04-08 06:35:08.965108: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2021-04-08 06:35:08.965586: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2299995000 Hz\n",
            "2021-04-08 06:35:08.982252: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-04-08 06:35:09.972742: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-04-08 06:35:10.186995: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "[Epoch 0/5] [Batch 0/150] [D loss: 0.307621] [G loss: 1443.703735] time: 0:00:07.217158\n",
            "[Epoch 0/5] [Batch 1/150] [D loss: 0.299213] [G loss: 863.580994] time: 0:00:12.631449\n",
            "[Epoch 0/5] [Batch 2/150] [D loss: 0.302959] [G loss: 632.372009] time: 0:00:13.969552\n",
            "[Epoch 0/5] [Batch 3/150] [D loss: 0.301475] [G loss: 554.444763] time: 0:00:15.306575\n",
            "[Epoch 0/5] [Batch 4/150] [D loss: 0.306913] [G loss: 867.182983] time: 0:00:16.695320\n",
            "[Epoch 0/5] [Batch 5/150] [D loss: 0.303642] [G loss: 597.518433] time: 0:00:18.031194\n",
            "[Epoch 0/5] [Batch 6/150] [D loss: 0.301521] [G loss: 510.694061] time: 0:00:19.380997\n",
            "[Epoch 0/5] [Batch 7/150] [D loss: 0.300187] [G loss: 415.048920] time: 0:00:20.727567\n",
            "[Epoch 0/5] [Batch 8/150] [D loss: 0.298612] [G loss: 524.902405] time: 0:00:22.051858\n",
            "[Epoch 0/5] [Batch 9/150] [D loss: 0.300871] [G loss: 541.888489] time: 0:00:23.391734\n",
            "[Epoch 0/5] [Batch 10/150] [D loss: 0.298088] [G loss: 619.275757] time: 0:00:24.725158\n",
            "[Epoch 0/5] [Batch 11/150] [D loss: 0.297799] [G loss: 710.265808] time: 0:00:26.061059\n",
            "[Epoch 0/5] [Batch 12/150] [D loss: 0.301909] [G loss: 915.749878] time: 0:00:27.396307\n",
            "[Epoch 0/5] [Batch 13/150] [D loss: 0.295827] [G loss: 379.608093] time: 0:00:28.740117\n",
            "[Epoch 0/5] [Batch 14/150] [D loss: 0.300664] [G loss: 483.492004] time: 0:00:30.078719\n",
            "[Epoch 0/5] [Batch 15/150] [D loss: 0.295443] [G loss: 372.447296] time: 0:00:31.563406\n",
            "[Epoch 0/5] [Batch 16/150] [D loss: 0.297910] [G loss: 478.981079] time: 0:00:32.913524\n",
            "[Epoch 0/5] [Batch 17/150] [D loss: 0.294720] [G loss: 396.861511] time: 0:00:34.244789\n",
            "[Epoch 0/5] [Batch 18/150] [D loss: 0.295087] [G loss: 307.034119] time: 0:00:35.593814\n",
            "[Epoch 0/5] [Batch 19/150] [D loss: 0.295572] [G loss: 353.796143] time: 0:00:36.946180\n",
            "[Epoch 0/5] [Batch 20/150] [D loss: 0.292328] [G loss: 649.302429] time: 0:00:38.279406\n",
            "[Epoch 0/5] [Batch 21/150] [D loss: 0.298332] [G loss: 410.004272] time: 0:00:39.618831\n",
            "[Epoch 0/5] [Batch 22/150] [D loss: 0.291289] [G loss: 493.216217] time: 0:00:40.943095\n",
            "[Epoch 0/5] [Batch 23/150] [D loss: 0.298720] [G loss: 580.642090] time: 0:00:42.320840\n",
            "[Epoch 0/5] [Batch 24/150] [D loss: 0.295988] [G loss: 502.963074] time: 0:00:43.671787\n",
            "[Epoch 0/5] [Batch 25/150] [D loss: 0.293003] [G loss: 479.039612] time: 0:00:45.013737\n",
            "[Epoch 0/5] [Batch 26/150] [D loss: 0.291856] [G loss: 460.754364] time: 0:00:47.620897\n",
            "[Epoch 0/5] [Batch 27/150] [D loss: 0.294037] [G loss: 453.764130] time: 0:00:48.962663\n",
            "[Epoch 0/5] [Batch 28/150] [D loss: 0.298442] [G loss: 334.323090] time: 0:00:50.298686\n",
            "[Epoch 0/5] [Batch 29/150] [D loss: 0.288225] [G loss: 363.873749] time: 0:00:51.650671\n",
            "[Epoch 0/5] [Batch 30/150] [D loss: 0.301935] [G loss: 468.269653] time: 0:00:52.972361\n",
            "[Epoch 0/5] [Batch 31/150] [D loss: 0.295249] [G loss: 340.842468] time: 0:00:54.327034\n",
            "[Epoch 0/5] [Batch 32/150] [D loss: 0.287131] [G loss: 522.077148] time: 0:00:55.671465\n",
            "[Epoch 0/5] [Batch 33/150] [D loss: 0.294170] [G loss: 455.978882] time: 0:00:57.008692\n",
            "[Epoch 0/5] [Batch 34/150] [D loss: 0.291313] [G loss: 608.012939] time: 0:00:58.346476\n",
            "[Epoch 0/5] [Batch 35/150] [D loss: 0.288436] [G loss: 486.506683] time: 0:00:59.688959\n",
            "[Epoch 0/5] [Batch 36/150] [D loss: 0.291074] [G loss: 687.759583] time: 0:01:01.043363\n",
            "[Epoch 0/5] [Batch 37/150] [D loss: 0.293772] [G loss: 790.710388] time: 0:01:02.402736\n",
            "[Epoch 0/5] [Batch 38/150] [D loss: 0.284542] [G loss: 386.930328] time: 0:01:03.738869\n",
            "[Epoch 0/5] [Batch 39/150] [D loss: 0.292601] [G loss: 575.633789] time: 0:01:05.097907\n",
            "[Epoch 0/5] [Batch 40/150] [D loss: 0.281920] [G loss: 508.699249] time: 0:01:06.606460\n",
            "[Epoch 0/5] [Batch 41/150] [D loss: 0.290464] [G loss: 572.283813] time: 0:01:07.962053\n",
            "[Epoch 0/5] [Batch 42/150] [D loss: 0.288254] [G loss: 312.567383] time: 0:01:09.317265\n",
            "[Epoch 0/5] [Batch 43/150] [D loss: 0.284597] [G loss: 532.273071] time: 0:01:10.656422\n",
            "[Epoch 0/5] [Batch 44/150] [D loss: 0.285973] [G loss: 591.253723] time: 0:01:12.046138\n",
            "[Epoch 0/5] [Batch 45/150] [D loss: 0.289239] [G loss: 711.674438] time: 0:01:13.403077\n",
            "[Epoch 0/5] [Batch 46/150] [D loss: 0.298433] [G loss: 658.207764] time: 0:01:14.735864\n",
            "[Epoch 0/5] [Batch 47/150] [D loss: 0.290725] [G loss: 985.313721] time: 0:01:16.077783\n",
            "[Epoch 0/5] [Batch 48/150] [D loss: 0.292470] [G loss: 471.922607] time: 0:01:17.427939\n",
            "[Epoch 0/5] [Batch 49/150] [D loss: 0.297571] [G loss: 570.362244] time: 0:01:18.767197\n",
            "[Epoch 0/5] [Batch 50/150] [D loss: 0.290964] [G loss: 502.009430] time: 0:01:20.116391\n",
            "[Epoch 0/5] [Batch 51/150] [D loss: 0.289231] [G loss: 266.897858] time: 0:01:22.441377\n",
            "[Epoch 0/5] [Batch 52/150] [D loss: 0.296353] [G loss: 618.730347] time: 0:01:23.797828\n",
            "[Epoch 0/5] [Batch 53/150] [D loss: 0.300679] [G loss: 491.735229] time: 0:01:25.144834\n",
            "[Epoch 0/5] [Batch 54/150] [D loss: 0.296714] [G loss: 440.786316] time: 0:01:26.488628\n",
            "[Epoch 0/5] [Batch 55/150] [D loss: 0.298573] [G loss: 743.944641] time: 0:01:27.894340\n",
            "[Epoch 0/5] [Batch 56/150] [D loss: 0.283975] [G loss: 266.816925] time: 0:01:29.242443\n",
            "[Epoch 0/5] [Batch 57/150] [D loss: 0.286993] [G loss: 601.839417] time: 0:01:30.788047\n",
            "[Epoch 0/5] [Batch 58/150] [D loss: 0.287812] [G loss: 387.570526] time: 0:01:32.144238\n",
            "[Epoch 0/5] [Batch 59/150] [D loss: 0.285783] [G loss: 462.645782] time: 0:01:33.503885\n",
            "[Epoch 0/5] [Batch 60/150] [D loss: 0.291165] [G loss: 468.144806] time: 0:01:34.832897\n",
            "[Epoch 0/5] [Batch 61/150] [D loss: 0.288338] [G loss: 581.354370] time: 0:01:36.181408\n",
            "[Epoch 0/5] [Batch 62/150] [D loss: 0.294390] [G loss: 488.716034] time: 0:01:37.528429\n",
            "[Epoch 0/5] [Batch 63/150] [D loss: 0.297768] [G loss: 567.511475] time: 0:01:38.875733\n",
            "[Epoch 0/5] [Batch 64/150] [D loss: 0.285448] [G loss: 308.810059] time: 0:01:40.222342\n",
            "[Epoch 0/5] [Batch 65/150] [D loss: 0.291717] [G loss: 514.215515] time: 0:01:41.708500\n",
            "[Epoch 0/5] [Batch 66/150] [D loss: 0.298055] [G loss: 578.762573] time: 0:01:43.070589\n",
            "[Epoch 0/5] [Batch 67/150] [D loss: 0.293117] [G loss: 739.154358] time: 0:01:44.432364\n",
            "[Epoch 0/5] [Batch 68/150] [D loss: 0.290944] [G loss: 499.116028] time: 0:01:45.789050\n",
            "[Epoch 0/5] [Batch 69/150] [D loss: 0.287900] [G loss: 211.323532] time: 0:01:47.140077\n",
            "[Epoch 0/5] [Batch 70/150] [D loss: 0.292899] [G loss: 604.334290] time: 0:01:48.484961\n",
            "[Epoch 0/5] [Batch 71/150] [D loss: 0.294829] [G loss: 334.448639] time: 0:01:49.831493\n",
            "[Epoch 0/5] [Batch 72/150] [D loss: 0.298418] [G loss: 727.915100] time: 0:01:51.230290\n",
            "[Epoch 0/5] [Batch 73/150] [D loss: 0.288180] [G loss: 339.482452] time: 0:01:52.581521\n",
            "[Epoch 0/5] [Batch 74/150] [D loss: 0.294759] [G loss: 425.944183] time: 0:01:53.918934\n",
            "[Epoch 0/5] [Batch 75/150] [D loss: 0.293344] [G loss: 606.624939] time: 0:01:55.282801\n",
            "[Epoch 0/5] [Batch 76/150] [D loss: 0.292512] [G loss: 406.643005] time: 0:01:57.604538\n",
            "[Epoch 0/5] [Batch 77/150] [D loss: 0.294348] [G loss: 249.058487] time: 0:01:58.934387\n",
            "[Epoch 0/5] [Batch 78/150] [D loss: 0.285983] [G loss: 517.374207] time: 0:02:00.295136\n",
            "[Epoch 0/5] [Batch 79/150] [D loss: 0.292397] [G loss: 603.628845] time: 0:02:01.636651\n",
            "[Epoch 0/5] [Batch 80/150] [D loss: 0.281517] [G loss: 462.817749] time: 0:02:02.976640\n",
            "[Epoch 0/5] [Batch 81/150] [D loss: 0.288092] [G loss: 411.182770] time: 0:02:04.321808\n",
            "[Epoch 0/5] [Batch 82/150] [D loss: 0.292636] [G loss: 549.663391] time: 0:02:05.656518\n",
            "[Epoch 0/5] [Batch 83/150] [D loss: 0.292639] [G loss: 542.497498] time: 0:02:06.987634\n",
            "[Epoch 0/5] [Batch 84/150] [D loss: 0.283143] [G loss: 417.945374] time: 0:02:08.322150\n",
            "[Epoch 0/5] [Batch 85/150] [D loss: 0.289026] [G loss: 388.291809] time: 0:02:09.645404\n",
            "[Epoch 0/5] [Batch 86/150] [D loss: 0.292153] [G loss: 312.359131] time: 0:02:10.992884\n",
            "[Epoch 0/5] [Batch 87/150] [D loss: 0.293484] [G loss: 504.123962] time: 0:02:12.350004\n",
            "[Epoch 0/5] [Batch 88/150] [D loss: 0.292920] [G loss: 205.031906] time: 0:02:13.706778\n",
            "[Epoch 0/5] [Batch 89/150] [D loss: 0.287094] [G loss: 295.291016] time: 0:02:15.041737\n",
            "[Epoch 0/5] [Batch 90/150] [D loss: 0.291093] [G loss: 334.932068] time: 0:02:16.545936\n",
            "[Epoch 0/5] [Batch 91/150] [D loss: 0.293829] [G loss: 411.248779] time: 0:02:17.884128\n",
            "[Epoch 0/5] [Batch 92/150] [D loss: 0.292091] [G loss: 256.812683] time: 0:02:19.237859\n",
            "[Epoch 0/5] [Batch 93/150] [D loss: 0.287252] [G loss: 505.385193] time: 0:02:20.601833\n",
            "[Epoch 0/5] [Batch 94/150] [D loss: 0.293761] [G loss: 574.195068] time: 0:02:21.938577\n",
            "[Epoch 0/5] [Batch 95/150] [D loss: 0.287231] [G loss: 262.543823] time: 0:02:23.272730\n",
            "[Epoch 0/5] [Batch 96/150] [D loss: 0.292607] [G loss: 376.922791] time: 0:02:24.618096\n",
            "[Epoch 0/5] [Batch 97/150] [D loss: 0.297867] [G loss: 646.557007] time: 0:02:25.943318\n",
            "[Epoch 0/5] [Batch 98/150] [D loss: 0.280749] [G loss: 462.895020] time: 0:02:27.279518\n",
            "[Epoch 0/5] [Batch 99/150] [D loss: 0.289707] [G loss: 490.283020] time: 0:02:28.611394\n",
            "[Epoch 0/5] [Batch 100/150] [D loss: 0.278672] [G loss: 300.982117] time: 0:02:29.954361\n",
            "[Epoch 0/5] [Batch 101/150] [D loss: 0.296401] [G loss: 646.719482] time: 0:02:32.612645\n",
            "[Epoch 0/5] [Batch 102/150] [D loss: 0.288482] [G loss: 381.202850] time: 0:02:33.935903\n",
            "[Epoch 0/5] [Batch 103/150] [D loss: 0.292553] [G loss: 460.873444] time: 0:02:35.266112\n",
            "[Epoch 0/5] [Batch 104/150] [D loss: 0.288725] [G loss: 460.687927] time: 0:02:36.597083\n",
            "[Epoch 0/5] [Batch 105/150] [D loss: 0.297336] [G loss: 601.629578] time: 0:02:37.946727\n",
            "[Epoch 0/5] [Batch 106/150] [D loss: 0.283227] [G loss: 257.910187] time: 0:02:39.276423\n",
            "[Epoch 0/5] [Batch 107/150] [D loss: 0.283666] [G loss: 619.319519] time: 0:02:40.592295\n",
            "[Epoch 0/5] [Batch 108/150] [D loss: 0.282666] [G loss: 284.284332] time: 0:02:41.910015\n",
            "[Epoch 0/5] [Batch 109/150] [D loss: 0.290350] [G loss: 425.740387] time: 0:02:43.247989\n",
            "[Epoch 0/5] [Batch 110/150] [D loss: 0.291778] [G loss: 565.504395] time: 0:02:44.582681\n",
            "[Epoch 0/5] [Batch 111/150] [D loss: 0.288652] [G loss: 575.494446] time: 0:02:45.926345\n",
            "[Epoch 0/5] [Batch 112/150] [D loss: 0.293211] [G loss: 178.202347] time: 0:02:47.236207\n",
            "[Epoch 0/5] [Batch 113/150] [D loss: 0.291213] [G loss: 514.733215] time: 0:02:48.580830\n",
            "[Epoch 0/5] [Batch 114/150] [D loss: 0.294550] [G loss: 657.508179] time: 0:02:49.921344\n",
            "[Epoch 0/5] [Batch 115/150] [D loss: 0.287254] [G loss: 318.558502] time: 0:02:51.397678\n",
            "[Epoch 0/5] [Batch 116/150] [D loss: 0.288286] [G loss: 372.046356] time: 0:02:52.732758\n",
            "[Epoch 0/5] [Batch 117/150] [D loss: 0.284839] [G loss: 259.152893] time: 0:02:54.057279\n",
            "[Epoch 0/5] [Batch 118/150] [D loss: 0.288782] [G loss: 221.684006] time: 0:02:55.389705\n",
            "[Epoch 0/5] [Batch 119/150] [D loss: 0.289186] [G loss: 217.187912] time: 0:02:56.718933\n",
            "[Epoch 0/5] [Batch 120/150] [D loss: 0.287836] [G loss: 554.994690] time: 0:02:58.043254\n",
            "[Epoch 0/5] [Batch 121/150] [D loss: 0.281549] [G loss: 414.341980] time: 0:02:59.378877\n",
            "[Epoch 0/5] [Batch 122/150] [D loss: 0.283869] [G loss: 320.944916] time: 0:03:00.710404\n",
            "[Epoch 0/5] [Batch 123/150] [D loss: 0.281785] [G loss: 440.359192] time: 0:03:02.050696\n",
            "[Epoch 0/5] [Batch 124/150] [D loss: 0.282385] [G loss: 468.238190] time: 0:03:03.390987\n",
            "[Epoch 0/5] [Batch 125/150] [D loss: 0.283184] [G loss: 195.937988] time: 0:03:04.724353\n",
            "[Epoch 0/5] [Batch 126/150] [D loss: 0.285450] [G loss: 287.747253] time: 0:03:07.000098\n",
            "[Epoch 0/5] [Batch 127/150] [D loss: 0.293827] [G loss: 598.059448] time: 0:03:08.361782\n",
            "[Epoch 0/5] [Batch 128/150] [D loss: 0.273655] [G loss: 306.917389] time: 0:03:09.701411\n",
            "[Epoch 0/5] [Batch 129/150] [D loss: 0.286919] [G loss: 376.948151] time: 0:03:11.602715\n",
            "[Epoch 0/5] [Batch 130/150] [D loss: 0.292223] [G loss: 564.775757] time: 0:03:12.948350\n",
            "[Epoch 0/5] [Batch 131/150] [D loss: 0.286191] [G loss: 351.322144] time: 0:03:14.288633\n",
            "[Epoch 0/5] [Batch 132/150] [D loss: 0.295253] [G loss: 571.575745] time: 0:03:15.608304\n",
            "[Epoch 0/5] [Batch 133/150] [D loss: 0.291543] [G loss: 494.986511] time: 0:03:16.945869\n",
            "[Epoch 0/5] [Batch 134/150] [D loss: 0.283135] [G loss: 480.459259] time: 0:03:18.288089\n",
            "[Epoch 0/5] [Batch 135/150] [D loss: 0.287906] [G loss: 463.345184] time: 0:03:19.614618\n",
            "[Epoch 0/5] [Batch 136/150] [D loss: 0.275779] [G loss: 543.550842] time: 0:03:20.973299\n",
            "[Epoch 0/5] [Batch 137/150] [D loss: 0.277974] [G loss: 415.388458] time: 0:03:22.314510\n",
            "[Epoch 0/5] [Batch 138/150] [D loss: 0.293477] [G loss: 808.936401] time: 0:03:23.648925\n",
            "[Epoch 0/5] [Batch 139/150] [D loss: 0.275859] [G loss: 279.416809] time: 0:03:24.979278\n",
            "[Epoch 0/5] [Batch 140/150] [D loss: 0.280047] [G loss: 290.605682] time: 0:03:26.443900\n",
            "[Epoch 0/5] [Batch 141/150] [D loss: 0.280008] [G loss: 410.900085] time: 0:03:28.596235\n",
            "[Epoch 0/5] [Batch 142/150] [D loss: 0.271109] [G loss: 397.760040] time: 0:03:29.952823\n",
            "[Epoch 0/5] [Batch 143/150] [D loss: 0.278577] [G loss: 400.664978] time: 0:03:31.287091\n",
            "[Epoch 0/5] [Batch 144/150] [D loss: 0.280371] [G loss: 452.939117] time: 0:03:32.618652\n",
            "[Epoch 0/5] [Batch 145/150] [D loss: 0.279955] [G loss: 502.712738] time: 0:03:34.018509\n",
            "[Epoch 0/5] [Batch 146/150] [D loss: 0.284092] [G loss: 446.308075] time: 0:03:35.351874\n",
            "[Epoch 0/5] [Batch 147/150] [D loss: 0.275556] [G loss: 292.216064] time: 0:03:36.707185\n",
            "[Epoch 0/5] [Batch 148/150] [D loss: 0.279821] [G loss: 537.251404] time: 0:03:38.048834\n",
            "[Epoch 1/5] [Batch 0/150] [D loss: 0.284970] [G loss: 662.126282] time: 0:03:39.404205\n",
            "[Epoch 1/5] [Batch 1/150] [D loss: 0.273047] [G loss: 262.286133] time: 0:03:41.463351\n",
            "[Epoch 1/5] [Batch 2/150] [D loss: 0.275634] [G loss: 250.034897] time: 0:03:42.931993\n",
            "[Epoch 1/5] [Batch 3/150] [D loss: 0.269232] [G loss: 343.349548] time: 0:03:44.272339\n",
            "[Epoch 1/5] [Batch 4/150] [D loss: 0.283514] [G loss: 660.454773] time: 0:03:45.661889\n",
            "[Epoch 1/5] [Batch 5/150] [D loss: 0.272838] [G loss: 451.228485] time: 0:03:47.006122\n",
            "[Epoch 1/5] [Batch 6/150] [D loss: 0.272814] [G loss: 326.808350] time: 0:03:48.345342\n",
            "[Epoch 1/5] [Batch 7/150] [D loss: 0.277003] [G loss: 271.905212] time: 0:03:49.691355\n",
            "[Epoch 1/5] [Batch 8/150] [D loss: 0.275696] [G loss: 365.560333] time: 0:03:51.030063\n",
            "[Epoch 1/5] [Batch 9/150] [D loss: 0.269913] [G loss: 397.988251] time: 0:03:52.372373\n",
            "[Epoch 1/5] [Batch 10/150] [D loss: 0.275404] [G loss: 477.595703] time: 0:03:53.707837\n",
            "[Epoch 1/5] [Batch 11/150] [D loss: 0.277802] [G loss: 537.770569] time: 0:03:55.047482\n",
            "[Epoch 1/5] [Batch 12/150] [D loss: 0.287102] [G loss: 778.180908] time: 0:03:56.402082\n",
            "[Epoch 1/5] [Batch 13/150] [D loss: 0.267591] [G loss: 291.831146] time: 0:03:57.747679\n",
            "[Epoch 1/5] [Batch 14/150] [D loss: 0.276672] [G loss: 335.439148] time: 0:03:59.070191\n",
            "[Epoch 1/5] [Batch 15/150] [D loss: 0.268567] [G loss: 269.922607] time: 0:04:00.407627\n",
            "[Epoch 1/5] [Batch 16/150] [D loss: 0.280206] [G loss: 370.406250] time: 0:04:01.747786\n",
            "[Epoch 1/5] [Batch 17/150] [D loss: 0.275772] [G loss: 298.537506] time: 0:04:03.072954\n",
            "[Epoch 1/5] [Batch 18/150] [D loss: 0.272547] [G loss: 225.595596] time: 0:04:04.557485\n",
            "[Epoch 1/5] [Batch 19/150] [D loss: 0.270608] [G loss: 256.391083] time: 0:04:05.901097\n",
            "[Epoch 1/5] [Batch 20/150] [D loss: 0.272050] [G loss: 519.914185] time: 0:04:07.249527\n",
            "[Epoch 1/5] [Batch 21/150] [D loss: 0.278342] [G loss: 301.259338] time: 0:04:08.593132\n",
            "[Epoch 1/5] [Batch 22/150] [D loss: 0.263132] [G loss: 368.414642] time: 0:04:09.942306\n",
            "[Epoch 1/5] [Batch 23/150] [D loss: 0.278198] [G loss: 441.440613] time: 0:04:11.270659\n",
            "[Epoch 1/5] [Batch 24/150] [D loss: 0.268273] [G loss: 383.437653] time: 0:04:12.616035\n",
            "[Epoch 1/5] [Batch 25/150] [D loss: 0.269117] [G loss: 344.122681] time: 0:04:13.943883\n",
            "[Epoch 1/5] [Batch 26/150] [D loss: 0.267775] [G loss: 356.476166] time: 0:04:15.976559\n",
            "[Epoch 1/5] [Batch 27/150] [D loss: 0.272344] [G loss: 335.440033] time: 0:04:17.445720\n",
            "[Epoch 1/5] [Batch 28/150] [D loss: 0.277615] [G loss: 256.709137] time: 0:04:18.778811\n",
            "[Epoch 1/5] [Batch 29/150] [D loss: 0.269941] [G loss: 226.893890] time: 0:04:20.124094\n",
            "[Epoch 1/5] [Batch 30/150] [D loss: 0.283316] [G loss: 321.631622] time: 0:04:21.462303\n",
            "[Epoch 1/5] [Batch 31/150] [D loss: 0.276342] [G loss: 249.997696] time: 0:04:22.802306\n",
            "[Epoch 1/5] [Batch 32/150] [D loss: 0.261486] [G loss: 415.091370] time: 0:04:24.146168\n",
            "[Epoch 1/5] [Batch 33/150] [D loss: 0.276115] [G loss: 399.705292] time: 0:04:25.473133\n",
            "[Epoch 1/5] [Batch 34/150] [D loss: 0.265777] [G loss: 510.777496] time: 0:04:26.805745\n",
            "[Epoch 1/5] [Batch 35/150] [D loss: 0.264698] [G loss: 365.513092] time: 0:04:28.139303\n",
            "[Epoch 1/5] [Batch 36/150] [D loss: 0.274495] [G loss: 560.264954] time: 0:04:29.499495\n",
            "[Epoch 1/5] [Batch 37/150] [D loss: 0.275223] [G loss: 614.868713] time: 0:04:30.842108\n",
            "[Epoch 1/5] [Batch 38/150] [D loss: 0.258615] [G loss: 360.380157] time: 0:04:32.183477\n",
            "[Epoch 1/5] [Batch 39/150] [D loss: 0.272790] [G loss: 430.517700] time: 0:04:33.519492\n",
            "[Epoch 1/5] [Batch 40/150] [D loss: 0.263792] [G loss: 385.118408] time: 0:04:34.851711\n",
            "[Epoch 1/5] [Batch 41/150] [D loss: 0.273477] [G loss: 430.366730] time: 0:04:36.199691\n",
            "[Epoch 1/5] [Batch 42/150] [D loss: 0.269795] [G loss: 262.402954] time: 0:04:37.547783\n",
            "[Epoch 1/5] [Batch 43/150] [D loss: 0.265801] [G loss: 437.436218] time: 0:04:39.033476\n",
            "[Epoch 1/5] [Batch 44/150] [D loss: 0.269144] [G loss: 501.039429] time: 0:04:40.407822\n",
            "[Epoch 1/5] [Batch 45/150] [D loss: 0.277771] [G loss: 581.004578] time: 0:04:41.748329\n",
            "[Epoch 1/5] [Batch 46/150] [D loss: 0.269434] [G loss: 509.806488] time: 0:04:43.087665\n",
            "[Epoch 1/5] [Batch 47/150] [D loss: 0.290835] [G loss: 982.794617] time: 0:04:44.419682\n",
            "[Epoch 1/5] [Batch 48/150] [D loss: 0.261340] [G loss: 370.749329] time: 0:04:45.781516\n",
            "[Epoch 1/5] [Batch 49/150] [D loss: 0.276017] [G loss: 494.259521] time: 0:04:47.121474\n",
            "[Epoch 1/5] [Batch 50/150] [D loss: 0.263025] [G loss: 445.525391] time: 0:04:48.457600\n",
            "[Epoch 1/5] [Batch 51/150] [D loss: 0.266798] [G loss: 225.495804] time: 0:04:50.800846\n",
            "[Epoch 1/5] [Batch 52/150] [D loss: 0.267122] [G loss: 511.421692] time: 0:04:52.281704\n",
            "[Epoch 1/5] [Batch 53/150] [D loss: 0.274186] [G loss: 397.425079] time: 0:04:53.624627\n",
            "[Epoch 1/5] [Batch 54/150] [D loss: 0.263517] [G loss: 339.828674] time: 0:04:54.965234\n",
            "[Epoch 1/5] [Batch 55/150] [D loss: 0.270445] [G loss: 616.134094] time: 0:04:56.379377\n",
            "[Epoch 1/5] [Batch 56/150] [D loss: 0.260657] [G loss: 214.069397] time: 0:04:57.725808\n",
            "[Epoch 1/5] [Batch 57/150] [D loss: 0.261284] [G loss: 478.576782] time: 0:04:59.271018\n",
            "[Epoch 1/5] [Batch 58/150] [D loss: 0.264554] [G loss: 314.078949] time: 0:05:00.612601\n",
            "[Epoch 1/5] [Batch 59/150] [D loss: 0.254900] [G loss: 374.222412] time: 0:05:01.948096\n",
            "[Epoch 1/5] [Batch 60/150] [D loss: 0.261268] [G loss: 361.683350] time: 0:05:03.283203\n",
            "[Epoch 1/5] [Batch 61/150] [D loss: 0.264558] [G loss: 476.468292] time: 0:05:04.625270\n",
            "[Epoch 1/5] [Batch 62/150] [D loss: 0.260850] [G loss: 388.352631] time: 0:05:05.968657\n",
            "[Epoch 1/5] [Batch 63/150] [D loss: 0.271577] [G loss: 439.273773] time: 0:05:07.310376\n",
            "[Epoch 1/5] [Batch 64/150] [D loss: 0.262260] [G loss: 239.365448] time: 0:05:08.650027\n",
            "[Epoch 1/5] [Batch 65/150] [D loss: 0.262350] [G loss: 403.956177] time: 0:05:09.986644\n",
            "[Epoch 1/5] [Batch 66/150] [D loss: 0.269645] [G loss: 466.453827] time: 0:05:11.316413\n",
            "[Epoch 1/5] [Batch 67/150] [D loss: 0.271939] [G loss: 624.700439] time: 0:05:12.664163\n",
            "[Epoch 1/5] [Batch 68/150] [D loss: 0.262893] [G loss: 403.972626] time: 0:05:14.148595\n",
            "[Epoch 1/5] [Batch 69/150] [D loss: 0.264092] [G loss: 232.455093] time: 0:05:15.484180\n",
            "[Epoch 1/5] [Batch 70/150] [D loss: 0.266053] [G loss: 523.012573] time: 0:05:16.829221\n",
            "[Epoch 1/5] [Batch 71/150] [D loss: 0.264068] [G loss: 274.395905] time: 0:05:18.157491\n",
            "[Epoch 1/5] [Batch 72/150] [D loss: 0.270930] [G loss: 593.927063] time: 0:05:19.548463\n",
            "[Epoch 1/5] [Batch 73/150] [D loss: 0.262146] [G loss: 236.757629] time: 0:05:20.925641\n",
            "[Epoch 1/5] [Batch 74/150] [D loss: 0.273498] [G loss: 327.557007] time: 0:05:22.254831\n",
            "[Epoch 1/5] [Batch 75/150] [D loss: 0.262743] [G loss: 471.124481] time: 0:05:23.590749\n",
            "[Epoch 1/5] [Batch 76/150] [D loss: 0.268643] [G loss: 280.935150] time: 0:05:25.601919\n",
            "[Epoch 1/5] [Batch 77/150] [D loss: 0.291463] [G loss: 166.786240] time: 0:05:27.069330\n",
            "[Epoch 1/5] [Batch 78/150] [D loss: 0.259453] [G loss: 402.428589] time: 0:05:28.392373\n",
            "[Epoch 1/5] [Batch 79/150] [D loss: 0.274074] [G loss: 450.837677] time: 0:05:29.733265\n",
            "[Epoch 1/5] [Batch 80/150] [D loss: 0.253412] [G loss: 366.909760] time: 0:05:31.080763\n",
            "[Epoch 1/5] [Batch 81/150] [D loss: 0.265665] [G loss: 301.223083] time: 0:05:32.432050\n",
            "[Epoch 1/5] [Batch 82/150] [D loss: 0.267676] [G loss: 410.549622] time: 0:05:33.771229\n",
            "[Epoch 1/5] [Batch 83/150] [D loss: 0.267212] [G loss: 394.913177] time: 0:05:35.106683\n",
            "[Epoch 1/5] [Batch 84/150] [D loss: 0.253063] [G loss: 342.839447] time: 0:05:36.442472\n",
            "[Epoch 1/5] [Batch 85/150] [D loss: 0.265413] [G loss: 290.954895] time: 0:05:37.779327\n",
            "[Epoch 1/5] [Batch 86/150] [D loss: 0.260168] [G loss: 244.456711] time: 0:05:39.123454\n",
            "[Epoch 1/5] [Batch 87/150] [D loss: 0.256963] [G loss: 388.557098] time: 0:05:40.468623\n",
            "[Epoch 1/5] [Batch 88/150] [D loss: 0.267830] [G loss: 163.674759] time: 0:05:41.815614\n",
            "[Epoch 1/5] [Batch 89/150] [D loss: 0.261803] [G loss: 225.621292] time: 0:05:43.152003\n",
            "[Epoch 1/5] [Batch 90/150] [D loss: 0.260296] [G loss: 275.653259] time: 0:05:44.492824\n",
            "[Epoch 1/5] [Batch 91/150] [D loss: 0.256817] [G loss: 309.928436] time: 0:05:45.829440\n",
            "[Epoch 1/5] [Batch 92/150] [D loss: 0.266175] [G loss: 202.111435] time: 0:05:47.184428\n",
            "[Epoch 1/5] [Batch 93/150] [D loss: 0.259230] [G loss: 383.064301] time: 0:05:48.672094\n",
            "[Epoch 1/5] [Batch 94/150] [D loss: 0.260488] [G loss: 447.876709] time: 0:05:50.010321\n",
            "[Epoch 1/5] [Batch 95/150] [D loss: 0.256227] [G loss: 198.645721] time: 0:05:51.352977\n",
            "[Epoch 1/5] [Batch 96/150] [D loss: 0.267508] [G loss: 283.162079] time: 0:05:52.684654\n",
            "[Epoch 1/5] [Batch 97/150] [D loss: 0.260887] [G loss: 496.145691] time: 0:05:54.014655\n",
            "[Epoch 1/5] [Batch 98/150] [D loss: 0.246021] [G loss: 386.304626] time: 0:05:55.364665\n",
            "[Epoch 1/5] [Batch 99/150] [D loss: 0.259648] [G loss: 383.116455] time: 0:05:56.706141\n",
            "[Epoch 1/5] [Batch 100/150] [D loss: 0.256652] [G loss: 267.813477] time: 0:05:58.032772\n",
            "[Epoch 1/5] [Batch 101/150] [D loss: 0.267889] [G loss: 485.725037] time: 0:06:00.197390\n",
            "[Epoch 1/5] [Batch 102/150] [D loss: 0.263316] [G loss: 306.863556] time: 0:06:01.697042\n",
            "[Epoch 1/5] [Batch 103/150] [D loss: 0.273539] [G loss: 368.907501] time: 0:06:03.041713\n",
            "[Epoch 1/5] [Batch 104/150] [D loss: 0.259074] [G loss: 353.057281] time: 0:06:04.390187\n",
            "[Epoch 1/5] [Batch 105/150] [D loss: 0.274381] [G loss: 475.517059] time: 0:06:05.756042\n",
            "[Epoch 1/5] [Batch 106/150] [D loss: 0.263252] [G loss: 217.722183] time: 0:06:07.097909\n",
            "[Epoch 1/5] [Batch 107/150] [D loss: 0.257151] [G loss: 518.186401] time: 0:06:08.445169\n",
            "[Epoch 1/5] [Batch 108/150] [D loss: 0.258488] [G loss: 212.594727] time: 0:06:09.790653\n",
            "[Epoch 1/5] [Batch 109/150] [D loss: 0.253140] [G loss: 344.249176] time: 0:06:11.113738\n",
            "[Epoch 1/5] [Batch 110/150] [D loss: 0.257930] [G loss: 508.178406] time: 0:06:12.469257\n",
            "[Epoch 1/5] [Batch 111/150] [D loss: 0.258270] [G loss: 446.287384] time: 0:06:13.803437\n",
            "[Epoch 1/5] [Batch 112/150] [D loss: 0.279907] [G loss: 111.660217] time: 0:06:15.118414\n",
            "[Epoch 1/5] [Batch 113/150] [D loss: 0.251966] [G loss: 406.777039] time: 0:06:16.452716\n",
            "[Epoch 1/5] [Batch 114/150] [D loss: 0.258309] [G loss: 529.876343] time: 0:06:17.791386\n",
            "[Epoch 1/5] [Batch 115/150] [D loss: 0.254386] [G loss: 255.845947] time: 0:06:19.132089\n",
            "[Epoch 1/5] [Batch 116/150] [D loss: 0.258503] [G loss: 322.987701] time: 0:06:20.504771\n",
            "[Epoch 1/5] [Batch 117/150] [D loss: 0.261216] [G loss: 202.624252] time: 0:06:21.846835\n",
            "[Epoch 1/5] [Batch 118/150] [D loss: 0.257459] [G loss: 178.581894] time: 0:06:23.322349\n",
            "[Epoch 1/5] [Batch 119/150] [D loss: 0.262973] [G loss: 166.165070] time: 0:06:24.659152\n",
            "[Epoch 1/5] [Batch 120/150] [D loss: 0.254197] [G loss: 451.129517] time: 0:06:25.992620\n",
            "[Epoch 1/5] [Batch 121/150] [D loss: 0.252421] [G loss: 342.161438] time: 0:06:27.333493\n",
            "[Epoch 1/5] [Batch 122/150] [D loss: 0.258772] [G loss: 226.685699] time: 0:06:28.671938\n",
            "[Epoch 1/5] [Batch 123/150] [D loss: 0.251308] [G loss: 348.688660] time: 0:06:30.010878\n",
            "[Epoch 1/5] [Batch 124/150] [D loss: 0.255282] [G loss: 367.034576] time: 0:06:31.334303\n",
            "[Epoch 1/5] [Batch 125/150] [D loss: 0.252339] [G loss: 146.808167] time: 0:06:32.676783\n",
            "[Epoch 1/5] [Batch 126/150] [D loss: 0.262224] [G loss: 213.231750] time: 0:06:34.897150\n",
            "[Epoch 1/5] [Batch 127/150] [D loss: 0.264499] [G loss: 513.286255] time: 0:06:36.397743\n",
            "[Epoch 1/5] [Batch 128/150] [D loss: 0.245465] [G loss: 271.223480] time: 0:06:37.728389\n",
            "[Epoch 1/5] [Batch 129/150] [D loss: 0.261364] [G loss: 304.692322] time: 0:06:39.618889\n",
            "[Epoch 1/5] [Batch 130/150] [D loss: 0.259050] [G loss: 452.139618] time: 0:06:40.966007\n",
            "[Epoch 1/5] [Batch 131/150] [D loss: 0.258136] [G loss: 260.729004] time: 0:06:42.316401\n",
            "[Epoch 1/5] [Batch 132/150] [D loss: 0.265019] [G loss: 448.138123] time: 0:06:43.655037\n",
            "[Epoch 1/5] [Batch 133/150] [D loss: 0.261453] [G loss: 367.668182] time: 0:06:44.991398\n",
            "[Epoch 1/5] [Batch 134/150] [D loss: 0.255816] [G loss: 371.313263] time: 0:06:46.327448\n",
            "[Epoch 1/5] [Batch 135/150] [D loss: 0.263213] [G loss: 361.773895] time: 0:06:47.670211\n",
            "[Epoch 1/5] [Batch 136/150] [D loss: 0.250019] [G loss: 440.008820] time: 0:06:49.033138\n",
            "[Epoch 1/5] [Batch 137/150] [D loss: 0.255035] [G loss: 319.973755] time: 0:06:50.376958\n",
            "[Epoch 1/5] [Batch 138/150] [D loss: 0.267400] [G loss: 665.633850] time: 0:06:51.731648\n",
            "[Epoch 1/5] [Batch 139/150] [D loss: 0.251968] [G loss: 223.611603] time: 0:06:53.066850\n",
            "[Epoch 1/5] [Batch 140/150] [D loss: 0.255422] [G loss: 246.225555] time: 0:06:54.412193\n",
            "[Epoch 1/5] [Batch 141/150] [D loss: 0.255407] [G loss: 330.970551] time: 0:06:56.561991\n",
            "[Epoch 1/5] [Batch 142/150] [D loss: 0.250277] [G loss: 311.896515] time: 0:06:57.900521\n",
            "[Epoch 1/5] [Batch 143/150] [D loss: 0.254073] [G loss: 314.097473] time: 0:06:59.400702\n",
            "[Epoch 1/5] [Batch 144/150] [D loss: 0.253036] [G loss: 346.312378] time: 0:07:00.724065\n",
            "[Epoch 1/5] [Batch 145/150] [D loss: 0.254821] [G loss: 414.271362] time: 0:07:02.150246\n",
            "[Epoch 1/5] [Batch 146/150] [D loss: 0.261281] [G loss: 336.352600] time: 0:07:03.492512\n",
            "[Epoch 1/5] [Batch 147/150] [D loss: 0.249866] [G loss: 239.324356] time: 0:07:04.831247\n",
            "[Epoch 1/5] [Batch 148/150] [D loss: 0.256712] [G loss: 427.895691] time: 0:07:06.176068\n",
            "[Epoch 2/5] [Batch 0/150] [D loss: 0.257737] [G loss: 527.455139] time: 0:07:07.512211\n",
            "[Epoch 2/5] [Batch 1/150] [D loss: 0.253201] [G loss: 211.877396] time: 0:07:09.589521\n",
            "[Epoch 2/5] [Batch 2/150] [D loss: 0.250567] [G loss: 233.146820] time: 0:07:10.920319\n",
            "[Epoch 2/5] [Batch 3/150] [D loss: 0.246306] [G loss: 274.589539] time: 0:07:12.267470\n",
            "[Epoch 2/5] [Batch 4/150] [D loss: 0.262012] [G loss: 579.932434] time: 0:07:13.780695\n",
            "[Epoch 2/5] [Batch 5/150] [D loss: 0.250150] [G loss: 375.282959] time: 0:07:15.125130\n",
            "[Epoch 2/5] [Batch 6/150] [D loss: 0.247009] [G loss: 269.662354] time: 0:07:16.465470\n",
            "[Epoch 2/5] [Batch 7/150] [D loss: 0.257721] [G loss: 221.169800] time: 0:07:17.819514\n",
            "[Epoch 2/5] [Batch 8/150] [D loss: 0.256498] [G loss: 314.598602] time: 0:07:19.167299\n",
            "[Epoch 2/5] [Batch 9/150] [D loss: 0.245693] [G loss: 340.415222] time: 0:07:20.518283\n",
            "[Epoch 2/5] [Batch 10/150] [D loss: 0.251617] [G loss: 405.589172] time: 0:07:21.866787\n",
            "[Epoch 2/5] [Batch 11/150] [D loss: 0.254115] [G loss: 415.943695] time: 0:07:23.201135\n",
            "[Epoch 2/5] [Batch 12/150] [D loss: 0.264719] [G loss: 627.116089] time: 0:07:24.555591\n",
            "[Epoch 2/5] [Batch 13/150] [D loss: 0.245641] [G loss: 256.662689] time: 0:07:25.908616\n",
            "[Epoch 2/5] [Batch 14/150] [D loss: 0.254798] [G loss: 279.644104] time: 0:07:27.248122\n",
            "[Epoch 2/5] [Batch 15/150] [D loss: 0.245789] [G loss: 254.582489] time: 0:07:28.574495\n",
            "[Epoch 2/5] [Batch 16/150] [D loss: 0.260004] [G loss: 332.048065] time: 0:07:29.903375\n",
            "[Epoch 2/5] [Batch 17/150] [D loss: 0.262559] [G loss: 243.097366] time: 0:07:31.218860\n",
            "[Epoch 2/5] [Batch 18/150] [D loss: 0.258388] [G loss: 218.918594] time: 0:07:32.681989\n",
            "[Epoch 2/5] [Batch 19/150] [D loss: 0.252968] [G loss: 210.178406] time: 0:07:34.016707\n",
            "[Epoch 2/5] [Batch 20/150] [D loss: 0.252233] [G loss: 452.029083] time: 0:07:35.346666\n",
            "[Epoch 2/5] [Batch 21/150] [D loss: 0.262014] [G loss: 252.840591] time: 0:07:36.681690\n",
            "[Epoch 2/5] [Batch 22/150] [D loss: 0.239738] [G loss: 302.304443] time: 0:07:38.003771\n",
            "[Epoch 2/5] [Batch 23/150] [D loss: 0.252726] [G loss: 330.768860] time: 0:07:39.343405\n",
            "[Epoch 2/5] [Batch 24/150] [D loss: 0.248135] [G loss: 328.169281] time: 0:07:40.689572\n",
            "[Epoch 2/5] [Batch 25/150] [D loss: 0.249806] [G loss: 283.526733] time: 0:07:42.013746\n",
            "[Epoch 2/5] [Batch 26/150] [D loss: 0.251612] [G loss: 295.726410] time: 0:07:44.159329\n",
            "[Epoch 2/5] [Batch 27/150] [D loss: 0.252292] [G loss: 289.179749] time: 0:07:45.482787\n",
            "[Epoch 2/5] [Batch 28/150] [D loss: 0.254178] [G loss: 247.423111] time: 0:07:46.812453\n",
            "[Epoch 2/5] [Batch 29/150] [D loss: 0.250857] [G loss: 183.123535] time: 0:07:48.274827\n",
            "[Epoch 2/5] [Batch 30/150] [D loss: 0.267493] [G loss: 242.450607] time: 0:07:49.599475\n",
            "[Epoch 2/5] [Batch 31/150] [D loss: 0.256448] [G loss: 196.009949] time: 0:07:50.935858\n",
            "[Epoch 2/5] [Batch 32/150] [D loss: 0.243223] [G loss: 367.631073] time: 0:07:52.273236\n",
            "[Epoch 2/5] [Batch 33/150] [D loss: 0.257311] [G loss: 338.328400] time: 0:07:53.599309\n",
            "[Epoch 2/5] [Batch 34/150] [D loss: 0.243320] [G loss: 437.401917] time: 0:07:54.942309\n",
            "[Epoch 2/5] [Batch 35/150] [D loss: 0.241766] [G loss: 315.785797] time: 0:07:56.270320\n",
            "[Epoch 2/5] [Batch 36/150] [D loss: 0.256272] [G loss: 473.300262] time: 0:07:57.600958\n",
            "[Epoch 2/5] [Batch 37/150] [D loss: 0.253292] [G loss: 504.236664] time: 0:07:58.960002\n",
            "[Epoch 2/5] [Batch 38/150] [D loss: 0.240697] [G loss: 342.030823] time: 0:08:00.296850\n",
            "[Epoch 2/5] [Batch 39/150] [D loss: 0.251991] [G loss: 347.100922] time: 0:08:01.633470\n",
            "[Epoch 2/5] [Batch 40/150] [D loss: 0.242770] [G loss: 346.812653] time: 0:08:02.978484\n",
            "[Epoch 2/5] [Batch 41/150] [D loss: 0.251095] [G loss: 351.584106] time: 0:08:04.319031\n",
            "[Epoch 2/5] [Batch 42/150] [D loss: 0.251138] [G loss: 216.271179] time: 0:08:05.646932\n",
            "[Epoch 2/5] [Batch 43/150] [D loss: 0.247215] [G loss: 371.665131] time: 0:08:06.998664\n",
            "[Epoch 2/5] [Batch 44/150] [D loss: 0.248183] [G loss: 419.420227] time: 0:08:08.514475\n",
            "[Epoch 2/5] [Batch 45/150] [D loss: 0.256222] [G loss: 509.319122] time: 0:08:09.843808\n",
            "[Epoch 2/5] [Batch 46/150] [D loss: 0.250283] [G loss: 429.697021] time: 0:08:11.172996\n",
            "[Epoch 2/5] [Batch 47/150] [D loss: 0.274318] [G loss: 972.846558] time: 0:08:12.515905\n",
            "[Epoch 2/5] [Batch 48/150] [D loss: 0.244980] [G loss: 322.463409] time: 0:08:13.862808\n",
            "[Epoch 2/5] [Batch 49/150] [D loss: 0.257665] [G loss: 419.951233] time: 0:08:15.201950\n",
            "[Epoch 2/5] [Batch 50/150] [D loss: 0.244427] [G loss: 388.391113] time: 0:08:16.544039\n",
            "[Epoch 2/5] [Batch 51/150] [D loss: 0.253804] [G loss: 205.875427] time: 0:08:18.769145\n",
            "[Epoch 2/5] [Batch 52/150] [D loss: 0.246084] [G loss: 420.627533] time: 0:08:20.094559\n",
            "[Epoch 2/5] [Batch 53/150] [D loss: 0.258105] [G loss: 333.909698] time: 0:08:21.436773\n",
            "[Epoch 2/5] [Batch 54/150] [D loss: 0.247413] [G loss: 305.117126] time: 0:08:22.768549\n",
            "[Epoch 2/5] [Batch 55/150] [D loss: 0.251432] [G loss: 548.441040] time: 0:08:24.165115\n",
            "[Epoch 2/5] [Batch 56/150] [D loss: 0.245673] [G loss: 181.154572] time: 0:08:25.633520\n",
            "[Epoch 2/5] [Batch 57/150] [D loss: 0.245526] [G loss: 416.013855] time: 0:08:27.162606\n",
            "[Epoch 2/5] [Batch 58/150] [D loss: 0.249160] [G loss: 247.240143] time: 0:08:28.487902\n",
            "[Epoch 2/5] [Batch 59/150] [D loss: 0.236749] [G loss: 320.474762] time: 0:08:29.808258\n",
            "[Epoch 2/5] [Batch 60/150] [D loss: 0.243324] [G loss: 320.663147] time: 0:08:31.135861\n",
            "[Epoch 2/5] [Batch 61/150] [D loss: 0.247359] [G loss: 429.742279] time: 0:08:32.475886\n",
            "[Epoch 2/5] [Batch 62/150] [D loss: 0.241273] [G loss: 387.805328] time: 0:08:33.793148\n",
            "[Epoch 2/5] [Batch 63/150] [D loss: 0.249045] [G loss: 383.062744] time: 0:08:35.111026\n",
            "[Epoch 2/5] [Batch 64/150] [D loss: 0.247917] [G loss: 192.143784] time: 0:08:36.437299\n",
            "[Epoch 2/5] [Batch 65/150] [D loss: 0.248078] [G loss: 346.329407] time: 0:08:37.769022\n",
            "[Epoch 2/5] [Batch 66/150] [D loss: 0.257581] [G loss: 415.242401] time: 0:08:39.103493\n",
            "[Epoch 2/5] [Batch 67/150] [D loss: 0.256981] [G loss: 553.339233] time: 0:08:40.438530\n",
            "[Epoch 2/5] [Batch 68/150] [D loss: 0.250857] [G loss: 366.384796] time: 0:08:41.763656\n",
            "[Epoch 2/5] [Batch 69/150] [D loss: 0.252351] [G loss: 187.761536] time: 0:08:43.089797\n",
            "[Epoch 2/5] [Batch 70/150] [D loss: 0.250839] [G loss: 462.417328] time: 0:08:44.538600\n",
            "[Epoch 2/5] [Batch 71/150] [D loss: 0.248556] [G loss: 245.743362] time: 0:08:45.873003\n",
            "[Epoch 2/5] [Batch 72/150] [D loss: 0.251465] [G loss: 525.514465] time: 0:08:47.255692\n",
            "[Epoch 2/5] [Batch 73/150] [D loss: 0.249875] [G loss: 203.206818] time: 0:08:48.597833\n",
            "[Epoch 2/5] [Batch 74/150] [D loss: 0.256173] [G loss: 264.949341] time: 0:08:49.930497\n",
            "[Epoch 2/5] [Batch 75/150] [D loss: 0.241244] [G loss: 413.994873] time: 0:08:51.281178\n",
            "[Epoch 2/5] [Batch 76/150] [D loss: 0.252940] [G loss: 229.368942] time: 0:08:54.977559\n",
            "[Epoch 2/5] [Batch 77/150] [D loss: 0.278718] [G loss: 147.569336] time: 0:08:56.308358\n",
            "[Epoch 2/5] [Batch 78/150] [D loss: 0.244168] [G loss: 371.666046] time: 0:08:57.643770\n",
            "[Epoch 2/5] [Batch 79/150] [D loss: 0.256691] [G loss: 377.468903] time: 0:08:58.980844\n",
            "[Epoch 2/5] [Batch 80/150] [D loss: 0.237285] [G loss: 312.774231] time: 0:09:00.324675\n",
            "[Epoch 2/5] [Batch 81/150] [D loss: 0.250371] [G loss: 241.589264] time: 0:09:01.678700\n",
            "[Epoch 2/5] [Batch 82/150] [D loss: 0.249499] [G loss: 350.449829] time: 0:09:03.150952\n",
            "[Epoch 2/5] [Batch 83/150] [D loss: 0.249506] [G loss: 340.561310] time: 0:09:04.480188\n",
            "[Epoch 2/5] [Batch 84/150] [D loss: 0.238175] [G loss: 315.082062] time: 0:09:05.805170\n",
            "[Epoch 2/5] [Batch 85/150] [D loss: 0.250426] [G loss: 269.944000] time: 0:09:07.121875\n",
            "[Epoch 2/5] [Batch 86/150] [D loss: 0.242759] [G loss: 219.686234] time: 0:09:08.453969\n",
            "[Epoch 2/5] [Batch 87/150] [D loss: 0.244171] [G loss: 337.505890] time: 0:09:09.782055\n",
            "[Epoch 2/5] [Batch 88/150] [D loss: 0.252067] [G loss: 146.099396] time: 0:09:11.112235\n",
            "[Epoch 2/5] [Batch 89/150] [D loss: 0.246344] [G loss: 194.436050] time: 0:09:12.459637\n",
            "[Epoch 2/5] [Batch 90/150] [D loss: 0.250956] [G loss: 243.232391] time: 0:09:13.803404\n",
            "[Epoch 2/5] [Batch 91/150] [D loss: 0.241105] [G loss: 277.820984] time: 0:09:15.141538\n",
            "[Epoch 2/5] [Batch 92/150] [D loss: 0.250897] [G loss: 183.775787] time: 0:09:16.483209\n",
            "[Epoch 2/5] [Batch 93/150] [D loss: 0.245211] [G loss: 335.766785] time: 0:09:17.822716\n",
            "[Epoch 2/5] [Batch 94/150] [D loss: 0.246166] [G loss: 410.904419] time: 0:09:19.162798\n",
            "[Epoch 2/5] [Batch 95/150] [D loss: 0.243842] [G loss: 169.373138] time: 0:09:20.508767\n",
            "[Epoch 2/5] [Batch 96/150] [D loss: 0.256996] [G loss: 243.395905] time: 0:09:21.846015\n",
            "[Epoch 2/5] [Batch 97/150] [D loss: 0.247903] [G loss: 452.367889] time: 0:09:23.290182\n",
            "[Epoch 2/5] [Batch 98/150] [D loss: 0.229321] [G loss: 349.487183] time: 0:09:24.627349\n",
            "[Epoch 2/5] [Batch 99/150] [D loss: 0.242846] [G loss: 340.074860] time: 0:09:25.953300\n",
            "[Epoch 2/5] [Batch 100/150] [D loss: 0.245103] [G loss: 248.519592] time: 0:09:27.284126\n",
            "[Epoch 2/5] [Batch 101/150] [D loss: 0.251865] [G loss: 419.451263] time: 0:09:30.922040\n",
            "[Epoch 2/5] [Batch 102/150] [D loss: 0.251309] [G loss: 280.453369] time: 0:09:32.251431\n",
            "[Epoch 2/5] [Batch 103/150] [D loss: 0.258999] [G loss: 316.783844] time: 0:09:33.584146\n",
            "[Epoch 2/5] [Batch 104/150] [D loss: 0.246703] [G loss: 300.692047] time: 0:09:34.898225\n",
            "[Epoch 2/5] [Batch 105/150] [D loss: 0.261662] [G loss: 430.949646] time: 0:09:36.232090\n",
            "[Epoch 2/5] [Batch 106/150] [D loss: 0.251940] [G loss: 221.547134] time: 0:09:37.568495\n",
            "[Epoch 2/5] [Batch 107/150] [D loss: 0.240672] [G loss: 454.709106] time: 0:09:38.899826\n",
            "[Epoch 2/5] [Batch 108/150] [D loss: 0.245521] [G loss: 188.049652] time: 0:09:40.363513\n",
            "[Epoch 2/5] [Batch 109/150] [D loss: 0.236855] [G loss: 299.884979] time: 0:09:41.712322\n",
            "[Epoch 2/5] [Batch 110/150] [D loss: 0.245666] [G loss: 462.572479] time: 0:09:43.037268\n",
            "[Epoch 2/5] [Batch 111/150] [D loss: 0.247711] [G loss: 386.116241] time: 0:09:44.369389\n",
            "[Epoch 2/5] [Batch 112/150] [D loss: 0.267977] [G loss: 90.877937] time: 0:09:45.667380\n",
            "[Epoch 2/5] [Batch 113/150] [D loss: 0.237261] [G loss: 365.377533] time: 0:09:46.997627\n",
            "[Epoch 2/5] [Batch 114/150] [D loss: 0.245124] [G loss: 492.969543] time: 0:09:48.316301\n",
            "[Epoch 2/5] [Batch 115/150] [D loss: 0.242086] [G loss: 216.685684] time: 0:09:49.647771\n",
            "[Epoch 2/5] [Batch 116/150] [D loss: 0.243402] [G loss: 273.942566] time: 0:09:50.982808\n",
            "[Epoch 2/5] [Batch 117/150] [D loss: 0.246888] [G loss: 186.787979] time: 0:09:52.327930\n",
            "[Epoch 2/5] [Batch 118/150] [D loss: 0.244788] [G loss: 170.892319] time: 0:09:53.676120\n",
            "[Epoch 2/5] [Batch 119/150] [D loss: 0.251342] [G loss: 143.931839] time: 0:09:55.034469\n",
            "[Epoch 2/5] [Batch 120/150] [D loss: 0.242579] [G loss: 402.281647] time: 0:09:56.371707\n",
            "[Epoch 2/5] [Batch 121/150] [D loss: 0.240335] [G loss: 306.504608] time: 0:09:57.704301\n",
            "[Epoch 2/5] [Batch 122/150] [D loss: 0.245749] [G loss: 202.188339] time: 0:09:59.036426\n",
            "[Epoch 2/5] [Batch 123/150] [D loss: 0.242450] [G loss: 325.486298] time: 0:10:00.382053\n",
            "[Epoch 2/5] [Batch 124/150] [D loss: 0.238285] [G loss: 326.347473] time: 0:10:01.857113\n",
            "[Epoch 2/5] [Batch 125/150] [D loss: 0.238316] [G loss: 137.910675] time: 0:10:03.184463\n",
            "[Epoch 2/5] [Batch 126/150] [D loss: 0.249297] [G loss: 179.781479] time: 0:10:05.281553\n",
            "[Epoch 2/5] [Batch 127/150] [D loss: 0.252186] [G loss: 462.554108] time: 0:10:06.635973\n",
            "[Epoch 2/5] [Batch 128/150] [D loss: 0.232372] [G loss: 242.879974] time: 0:10:07.975696\n",
            "[Epoch 2/5] [Batch 129/150] [D loss: 0.247911] [G loss: 254.128204] time: 0:10:09.837939\n",
            "[Epoch 2/5] [Batch 130/150] [D loss: 0.244053] [G loss: 406.444366] time: 0:10:11.188926\n",
            "[Epoch 2/5] [Batch 131/150] [D loss: 0.247459] [G loss: 234.996170] time: 0:10:12.533243\n",
            "[Epoch 2/5] [Batch 132/150] [D loss: 0.252058] [G loss: 400.858887] time: 0:10:13.847270\n",
            "[Epoch 2/5] [Batch 133/150] [D loss: 0.245450] [G loss: 328.202942] time: 0:10:15.175271\n",
            "[Epoch 2/5] [Batch 134/150] [D loss: 0.243449] [G loss: 328.390839] time: 0:10:16.642209\n",
            "[Epoch 2/5] [Batch 135/150] [D loss: 0.252071] [G loss: 326.916260] time: 0:10:17.960846\n",
            "[Epoch 2/5] [Batch 136/150] [D loss: 0.238431] [G loss: 390.762024] time: 0:10:19.302779\n",
            "[Epoch 2/5] [Batch 137/150] [D loss: 0.242774] [G loss: 279.849426] time: 0:10:20.643584\n",
            "[Epoch 2/5] [Batch 138/150] [D loss: 0.255621] [G loss: 572.911804] time: 0:10:21.985188\n",
            "[Epoch 2/5] [Batch 139/150] [D loss: 0.237744] [G loss: 195.151764] time: 0:10:23.317700\n",
            "[Epoch 2/5] [Batch 140/150] [D loss: 0.243051] [G loss: 246.504837] time: 0:10:24.656327\n",
            "[Epoch 2/5] [Batch 141/150] [D loss: 0.240509] [G loss: 284.243774] time: 0:10:26.799439\n",
            "[Epoch 2/5] [Batch 142/150] [D loss: 0.238417] [G loss: 267.893707] time: 0:10:28.144179\n",
            "[Epoch 2/5] [Batch 143/150] [D loss: 0.241370] [G loss: 265.681091] time: 0:10:29.486922\n",
            "[Epoch 2/5] [Batch 144/150] [D loss: 0.235806] [G loss: 297.329681] time: 0:10:30.813408\n",
            "[Epoch 2/5] [Batch 145/150] [D loss: 0.240438] [G loss: 376.596436] time: 0:10:32.227146\n",
            "[Epoch 2/5] [Batch 146/150] [D loss: 0.249296] [G loss: 286.618011] time: 0:10:33.549759\n",
            "[Epoch 2/5] [Batch 147/150] [D loss: 0.236772] [G loss: 209.337448] time: 0:10:34.876545\n",
            "[Epoch 2/5] [Batch 148/150] [D loss: 0.247184] [G loss: 376.176331] time: 0:10:36.222795\n",
            "[Epoch 3/5] [Batch 0/150] [D loss: 0.245764] [G loss: 447.537140] time: 0:10:37.685858\n",
            "[Epoch 3/5] [Batch 1/150] [D loss: 0.241755] [G loss: 189.102341] time: 0:10:39.873109\n",
            "[Epoch 3/5] [Batch 2/150] [D loss: 0.238448] [G loss: 227.153259] time: 0:10:41.207495\n",
            "[Epoch 3/5] [Batch 3/150] [D loss: 0.232708] [G loss: 245.392502] time: 0:10:42.540877\n",
            "[Epoch 3/5] [Batch 4/150] [D loss: 0.246939] [G loss: 496.312805] time: 0:10:43.944429\n",
            "[Epoch 3/5] [Batch 5/150] [D loss: 0.238141] [G loss: 329.218262] time: 0:10:45.286478\n",
            "[Epoch 3/5] [Batch 6/150] [D loss: 0.233296] [G loss: 231.147858] time: 0:10:46.635375\n",
            "[Epoch 3/5] [Batch 7/150] [D loss: 0.246506] [G loss: 192.328308] time: 0:10:47.976657\n",
            "[Epoch 3/5] [Batch 8/150] [D loss: 0.246189] [G loss: 279.978760] time: 0:10:49.300493\n",
            "[Epoch 3/5] [Batch 9/150] [D loss: 0.232988] [G loss: 297.680115] time: 0:10:50.621670\n",
            "[Epoch 3/5] [Batch 10/150] [D loss: 0.242716] [G loss: 376.747406] time: 0:10:51.996436\n",
            "[Epoch 3/5] [Batch 11/150] [D loss: 0.243687] [G loss: 368.182983] time: 0:10:53.332914\n",
            "[Epoch 3/5] [Batch 12/150] [D loss: 0.251216] [G loss: 584.191589] time: 0:10:54.787645\n",
            "[Epoch 3/5] [Batch 13/150] [D loss: 0.236194] [G loss: 215.348129] time: 0:10:56.118826\n",
            "[Epoch 3/5] [Batch 14/150] [D loss: 0.240865] [G loss: 258.785919] time: 0:10:57.452283\n",
            "[Epoch 3/5] [Batch 15/150] [D loss: 0.234020] [G loss: 242.401688] time: 0:10:58.804950\n",
            "[Epoch 3/5] [Batch 16/150] [D loss: 0.243265] [G loss: 293.959656] time: 0:11:00.130447\n",
            "[Epoch 3/5] [Batch 17/150] [D loss: 0.246619] [G loss: 199.097763] time: 0:11:01.458664\n",
            "[Epoch 3/5] [Batch 18/150] [D loss: 0.246216] [G loss: 216.116196] time: 0:11:02.786854\n",
            "[Epoch 3/5] [Batch 19/150] [D loss: 0.236974] [G loss: 191.024460] time: 0:11:04.107259\n",
            "[Epoch 3/5] [Batch 20/150] [D loss: 0.239520] [G loss: 397.578186] time: 0:11:05.438049\n",
            "[Epoch 3/5] [Batch 21/150] [D loss: 0.247742] [G loss: 223.807343] time: 0:11:06.755688\n",
            "[Epoch 3/5] [Batch 22/150] [D loss: 0.227699] [G loss: 281.300140] time: 0:11:08.089926\n",
            "[Epoch 3/5] [Batch 23/150] [D loss: 0.251003] [G loss: 324.416382] time: 0:11:09.419920\n",
            "[Epoch 3/5] [Batch 24/150] [D loss: 0.238401] [G loss: 297.024811] time: 0:11:10.764797\n",
            "[Epoch 3/5] [Batch 25/150] [D loss: 0.242414] [G loss: 247.497208] time: 0:11:12.079532\n",
            "[Epoch 3/5] [Batch 26/150] [D loss: 0.243106] [G loss: 264.777222] time: 0:11:14.283206\n",
            "[Epoch 3/5] [Batch 27/150] [D loss: 0.242455] [G loss: 261.983063] time: 0:11:15.605694\n",
            "[Epoch 3/5] [Batch 28/150] [D loss: 0.241391] [G loss: 238.145950] time: 0:11:16.932435\n",
            "[Epoch 3/5] [Batch 29/150] [D loss: 0.241451] [G loss: 163.635757] time: 0:11:18.279746\n",
            "[Epoch 3/5] [Batch 30/150] [D loss: 0.257027] [G loss: 208.533157] time: 0:11:19.594040\n",
            "[Epoch 3/5] [Batch 31/150] [D loss: 0.243185] [G loss: 180.892868] time: 0:11:20.912587\n",
            "[Epoch 3/5] [Batch 32/150] [D loss: 0.233559] [G loss: 320.510773] time: 0:11:22.251516\n",
            "[Epoch 3/5] [Batch 33/150] [D loss: 0.247774] [G loss: 312.771729] time: 0:11:23.571995\n",
            "[Epoch 3/5] [Batch 34/150] [D loss: 0.229311] [G loss: 381.432709] time: 0:11:24.899631\n",
            "[Epoch 3/5] [Batch 35/150] [D loss: 0.230399] [G loss: 272.640747] time: 0:11:26.232545\n",
            "[Epoch 3/5] [Batch 36/150] [D loss: 0.245834] [G loss: 415.277740] time: 0:11:27.570797\n",
            "[Epoch 3/5] [Batch 37/150] [D loss: 0.239914] [G loss: 431.319916] time: 0:11:28.905556\n",
            "[Epoch 3/5] [Batch 38/150] [D loss: 0.230287] [G loss: 290.202484] time: 0:11:30.383318\n",
            "[Epoch 3/5] [Batch 39/150] [D loss: 0.240789] [G loss: 303.807159] time: 0:11:31.708759\n",
            "[Epoch 3/5] [Batch 40/150] [D loss: 0.232190] [G loss: 302.633575] time: 0:11:33.048244\n",
            "[Epoch 3/5] [Batch 41/150] [D loss: 0.239544] [G loss: 321.761078] time: 0:11:34.388279\n",
            "[Epoch 3/5] [Batch 42/150] [D loss: 0.238787] [G loss: 199.533951] time: 0:11:35.725510\n",
            "[Epoch 3/5] [Batch 43/150] [D loss: 0.232154] [G loss: 330.969482] time: 0:11:37.059504\n",
            "[Epoch 3/5] [Batch 44/150] [D loss: 0.239695] [G loss: 377.849915] time: 0:11:38.441117\n",
            "[Epoch 3/5] [Batch 45/150] [D loss: 0.255002] [G loss: 455.093811] time: 0:11:39.778415\n",
            "[Epoch 3/5] [Batch 46/150] [D loss: 0.246770] [G loss: 352.507507] time: 0:11:41.104701\n",
            "[Epoch 3/5] [Batch 47/150] [D loss: 0.269658] [G loss: 1049.737793] time: 0:11:42.457665\n",
            "[Epoch 3/5] [Batch 48/150] [D loss: 0.237581] [G loss: 281.648254] time: 0:11:43.810028\n",
            "[Epoch 3/5] [Batch 49/150] [D loss: 0.248545] [G loss: 366.979828] time: 0:11:45.131948\n",
            "[Epoch 3/5] [Batch 50/150] [D loss: 0.237869] [G loss: 338.111053] time: 0:11:46.473939\n",
            "[Epoch 3/5] [Batch 51/150] [D loss: 0.246058] [G loss: 214.374146] time: 0:11:48.607459\n",
            "[Epoch 3/5] [Batch 52/150] [D loss: 0.237578] [G loss: 381.902924] time: 0:11:49.925559\n",
            "[Epoch 3/5] [Batch 53/150] [D loss: 0.251876] [G loss: 295.639252] time: 0:11:51.253914\n",
            "[Epoch 3/5] [Batch 54/150] [D loss: 0.241787] [G loss: 281.880096] time: 0:11:52.600857\n",
            "[Epoch 3/5] [Batch 55/150] [D loss: 0.241669] [G loss: 491.922363] time: 0:11:54.005050\n",
            "[Epoch 3/5] [Batch 56/150] [D loss: 0.233851] [G loss: 158.756531] time: 0:11:55.336933\n",
            "[Epoch 3/5] [Batch 57/150] [D loss: 0.234029] [G loss: 365.441437] time: 0:11:56.896179\n",
            "[Epoch 3/5] [Batch 58/150] [D loss: 0.238651] [G loss: 221.759323] time: 0:11:58.228651\n",
            "[Epoch 3/5] [Batch 59/150] [D loss: 0.226511] [G loss: 297.412750] time: 0:11:59.562038\n",
            "[Epoch 3/5] [Batch 60/150] [D loss: 0.236064] [G loss: 288.059357] time: 0:12:00.883012\n",
            "[Epoch 3/5] [Batch 61/150] [D loss: 0.236779] [G loss: 391.896729] time: 0:12:02.225079\n",
            "[Epoch 3/5] [Batch 62/150] [D loss: 0.230328] [G loss: 350.359589] time: 0:12:03.571936\n",
            "[Epoch 3/5] [Batch 63/150] [D loss: 0.238866] [G loss: 350.720062] time: 0:12:04.916724\n",
            "[Epoch 3/5] [Batch 64/150] [D loss: 0.239257] [G loss: 182.971786] time: 0:12:06.398227\n",
            "[Epoch 3/5] [Batch 65/150] [D loss: 0.238906] [G loss: 316.760010] time: 0:12:07.726470\n",
            "[Epoch 3/5] [Batch 66/150] [D loss: 0.249086] [G loss: 365.126465] time: 0:12:09.077978\n",
            "[Epoch 3/5] [Batch 67/150] [D loss: 0.248412] [G loss: 486.526672] time: 0:12:10.417946\n",
            "[Epoch 3/5] [Batch 68/150] [D loss: 0.238766] [G loss: 331.270355] time: 0:12:11.755985\n",
            "[Epoch 3/5] [Batch 69/150] [D loss: 0.245478] [G loss: 163.287766] time: 0:12:13.105527\n",
            "[Epoch 3/5] [Batch 70/150] [D loss: 0.241192] [G loss: 422.783600] time: 0:12:14.433373\n",
            "[Epoch 3/5] [Batch 71/150] [D loss: 0.239432] [G loss: 219.687469] time: 0:12:15.747105\n",
            "[Epoch 3/5] [Batch 72/150] [D loss: 0.243145] [G loss: 473.674622] time: 0:12:17.140167\n",
            "[Epoch 3/5] [Batch 73/150] [D loss: 0.238017] [G loss: 181.638062] time: 0:12:18.467483\n",
            "[Epoch 3/5] [Batch 74/150] [D loss: 0.248192] [G loss: 222.231506] time: 0:12:19.778721\n",
            "[Epoch 3/5] [Batch 75/150] [D loss: 0.231681] [G loss: 375.948822] time: 0:12:21.124416\n",
            "[Epoch 3/5] [Batch 76/150] [D loss: 0.247263] [G loss: 197.776703] time: 0:12:23.296234\n",
            "[Epoch 3/5] [Batch 77/150] [D loss: 0.278940] [G loss: 141.047974] time: 0:12:24.621328\n",
            "[Epoch 3/5] [Batch 78/150] [D loss: 0.237219] [G loss: 330.076477] time: 0:12:25.945425\n",
            "[Epoch 3/5] [Batch 79/150] [D loss: 0.247732] [G loss: 326.236755] time: 0:12:27.276701\n",
            "[Epoch 3/5] [Batch 80/150] [D loss: 0.229762] [G loss: 294.682404] time: 0:12:28.614241\n",
            "[Epoch 3/5] [Batch 81/150] [D loss: 0.240252] [G loss: 206.514786] time: 0:12:29.935651\n",
            "[Epoch 3/5] [Batch 82/150] [D loss: 0.239172] [G loss: 315.135895] time: 0:12:31.273273\n",
            "[Epoch 3/5] [Batch 83/150] [D loss: 0.239543] [G loss: 300.289886] time: 0:12:32.618837\n",
            "[Epoch 3/5] [Batch 84/150] [D loss: 0.231065] [G loss: 282.643585] time: 0:12:33.944184\n",
            "[Epoch 3/5] [Batch 85/150] [D loss: 0.250985] [G loss: 247.039886] time: 0:12:35.260099\n",
            "[Epoch 3/5] [Batch 86/150] [D loss: 0.232448] [G loss: 195.572418] time: 0:12:36.593660\n",
            "[Epoch 3/5] [Batch 87/150] [D loss: 0.238516] [G loss: 312.830017] time: 0:12:37.923047\n",
            "[Epoch 3/5] [Batch 88/150] [D loss: 0.244065] [G loss: 129.075531] time: 0:12:39.245287\n",
            "[Epoch 3/5] [Batch 89/150] [D loss: 0.236300] [G loss: 178.936203] time: 0:12:40.702655\n",
            "[Epoch 3/5] [Batch 90/150] [D loss: 0.249955] [G loss: 225.214020] time: 0:12:42.027370\n",
            "[Epoch 3/5] [Batch 91/150] [D loss: 0.229579] [G loss: 247.122574] time: 0:12:43.391812\n",
            "[Epoch 3/5] [Batch 92/150] [D loss: 0.242071] [G loss: 173.908859] time: 0:12:44.723516\n",
            "[Epoch 3/5] [Batch 93/150] [D loss: 0.235790] [G loss: 303.583618] time: 0:12:46.044288\n",
            "[Epoch 3/5] [Batch 94/150] [D loss: 0.237245] [G loss: 371.256134] time: 0:12:47.380392\n",
            "[Epoch 3/5] [Batch 95/150] [D loss: 0.235481] [G loss: 156.768204] time: 0:12:48.696016\n",
            "[Epoch 3/5] [Batch 96/150] [D loss: 0.250341] [G loss: 224.399429] time: 0:12:50.020318\n",
            "[Epoch 3/5] [Batch 97/150] [D loss: 0.241059] [G loss: 426.468079] time: 0:12:51.350909\n",
            "[Epoch 3/5] [Batch 98/150] [D loss: 0.220249] [G loss: 313.316498] time: 0:12:52.693477\n",
            "[Epoch 3/5] [Batch 99/150] [D loss: 0.235395] [G loss: 296.562744] time: 0:12:54.021218\n",
            "[Epoch 3/5] [Batch 100/150] [D loss: 0.236702] [G loss: 247.054672] time: 0:12:55.353262\n",
            "[Epoch 3/5] [Batch 101/150] [D loss: 0.245827] [G loss: 380.438812] time: 0:12:57.910520\n",
            "[Epoch 3/5] [Batch 102/150] [D loss: 0.236568] [G loss: 247.469498] time: 0:12:59.223411\n",
            "[Epoch 3/5] [Batch 103/150] [D loss: 0.253311] [G loss: 279.890930] time: 0:13:00.542781\n",
            "[Epoch 3/5] [Batch 104/150] [D loss: 0.238298] [G loss: 272.620758] time: 0:13:01.886540\n",
            "[Epoch 3/5] [Batch 105/150] [D loss: 0.250155] [G loss: 394.773102] time: 0:13:03.225048\n",
            "[Epoch 3/5] [Batch 106/150] [D loss: 0.247060] [G loss: 207.902145] time: 0:13:04.550647\n",
            "[Epoch 3/5] [Batch 107/150] [D loss: 0.231797] [G loss: 420.889648] time: 0:13:05.898795\n",
            "[Epoch 3/5] [Batch 108/150] [D loss: 0.236440] [G loss: 172.803680] time: 0:13:07.233285\n",
            "[Epoch 3/5] [Batch 109/150] [D loss: 0.228456] [G loss: 270.495636] time: 0:13:08.561812\n",
            "[Epoch 3/5] [Batch 110/150] [D loss: 0.239239] [G loss: 434.678741] time: 0:13:09.894803\n",
            "[Epoch 3/5] [Batch 111/150] [D loss: 0.238403] [G loss: 368.860138] time: 0:13:11.231024\n",
            "[Epoch 3/5] [Batch 112/150] [D loss: 0.264671] [G loss: 75.594383] time: 0:13:12.567636\n",
            "[Epoch 3/5] [Batch 113/150] [D loss: 0.230297] [G loss: 339.228699] time: 0:13:13.902088\n",
            "[Epoch 3/5] [Batch 114/150] [D loss: 0.239502] [G loss: 458.353729] time: 0:13:15.392098\n",
            "[Epoch 3/5] [Batch 115/150] [D loss: 0.234606] [G loss: 197.342926] time: 0:13:16.728644\n",
            "[Epoch 3/5] [Batch 116/150] [D loss: 0.236134] [G loss: 253.506836] time: 0:13:18.061194\n",
            "[Epoch 3/5] [Batch 117/150] [D loss: 0.242882] [G loss: 181.197800] time: 0:13:19.395459\n",
            "[Epoch 3/5] [Batch 118/150] [D loss: 0.238197] [G loss: 175.176331] time: 0:13:20.723542\n",
            "[Epoch 3/5] [Batch 119/150] [D loss: 0.245869] [G loss: 138.091202] time: 0:13:22.065486\n",
            "[Epoch 3/5] [Batch 120/150] [D loss: 0.241420] [G loss: 363.084198] time: 0:13:23.406829\n",
            "[Epoch 3/5] [Batch 121/150] [D loss: 0.234335] [G loss: 294.289612] time: 0:13:24.736884\n",
            "[Epoch 3/5] [Batch 122/150] [D loss: 0.238517] [G loss: 193.460358] time: 0:13:26.061859\n",
            "[Epoch 3/5] [Batch 123/150] [D loss: 0.240775] [G loss: 304.666534] time: 0:13:27.394856\n",
            "[Epoch 3/5] [Batch 124/150] [D loss: 0.237578] [G loss: 308.322113] time: 0:13:28.725814\n",
            "[Epoch 3/5] [Batch 125/150] [D loss: 0.234211] [G loss: 127.083290] time: 0:13:30.053857\n",
            "[Epoch 3/5] [Batch 126/150] [D loss: 0.242998] [G loss: 170.032501] time: 0:13:32.293201\n",
            "[Epoch 3/5] [Batch 127/150] [D loss: 0.244118] [G loss: 441.110596] time: 0:13:33.657706\n",
            "[Epoch 3/5] [Batch 128/150] [D loss: 0.227094] [G loss: 210.610794] time: 0:13:35.000835\n",
            "[Epoch 3/5] [Batch 129/150] [D loss: 0.242215] [G loss: 234.967819] time: 0:13:36.877126\n",
            "[Epoch 3/5] [Batch 130/150] [D loss: 0.236393] [G loss: 374.697723] time: 0:13:38.211991\n",
            "[Epoch 3/5] [Batch 131/150] [D loss: 0.239122] [G loss: 226.231323] time: 0:13:39.542326\n",
            "[Epoch 3/5] [Batch 132/150] [D loss: 0.243929] [G loss: 369.598236] time: 0:13:40.850990\n",
            "[Epoch 3/5] [Batch 133/150] [D loss: 0.235307] [G loss: 274.672455] time: 0:13:42.189554\n",
            "[Epoch 3/5] [Batch 134/150] [D loss: 0.234666] [G loss: 313.922577] time: 0:13:43.534530\n",
            "[Epoch 3/5] [Batch 135/150] [D loss: 0.234583] [G loss: 299.101196] time: 0:13:44.868856\n",
            "[Epoch 3/5] [Batch 136/150] [D loss: 0.230046] [G loss: 373.624939] time: 0:13:46.227303\n",
            "[Epoch 3/5] [Batch 137/150] [D loss: 0.235126] [G loss: 257.919128] time: 0:13:47.555708\n",
            "[Epoch 3/5] [Batch 138/150] [D loss: 0.247745] [G loss: 523.205566] time: 0:13:48.872084\n",
            "[Epoch 3/5] [Batch 139/150] [D loss: 0.230752] [G loss: 171.021729] time: 0:13:50.346644\n",
            "[Epoch 3/5] [Batch 140/150] [D loss: 0.236564] [G loss: 234.642731] time: 0:13:51.671182\n",
            "[Epoch 3/5] [Batch 141/150] [D loss: 0.233911] [G loss: 265.158936] time: 0:13:53.799314\n",
            "[Epoch 3/5] [Batch 142/150] [D loss: 0.231336] [G loss: 249.867477] time: 0:13:55.126655\n",
            "[Epoch 3/5] [Batch 143/150] [D loss: 0.234807] [G loss: 247.512161] time: 0:13:56.455055\n",
            "[Epoch 3/5] [Batch 144/150] [D loss: 0.228007] [G loss: 267.067474] time: 0:13:57.783197\n",
            "[Epoch 3/5] [Batch 145/150] [D loss: 0.234990] [G loss: 340.133484] time: 0:13:59.181715\n",
            "[Epoch 3/5] [Batch 146/150] [D loss: 0.238877] [G loss: 263.939819] time: 0:14:00.518650\n",
            "[Epoch 3/5] [Batch 147/150] [D loss: 0.229325] [G loss: 197.513916] time: 0:14:01.863240\n",
            "[Epoch 3/5] [Batch 148/150] [D loss: 0.239694] [G loss: 337.111938] time: 0:14:03.210943\n",
            "[Epoch 4/5] [Batch 0/150] [D loss: 0.237810] [G loss: 418.657104] time: 0:14:04.562848\n",
            "[Epoch 4/5] [Batch 1/150] [D loss: 0.237119] [G loss: 180.655487] time: 0:14:06.819463\n",
            "[Epoch 4/5] [Batch 2/150] [D loss: 0.229600] [G loss: 206.500244] time: 0:14:08.139629\n",
            "[Epoch 4/5] [Batch 3/150] [D loss: 0.226352] [G loss: 239.548233] time: 0:14:09.470431\n",
            "[Epoch 4/5] [Batch 4/150] [D loss: 0.239448] [G loss: 476.398254] time: 0:14:10.862816\n",
            "[Epoch 4/5] [Batch 5/150] [D loss: 0.229832] [G loss: 299.973267] time: 0:14:12.200125\n",
            "[Epoch 4/5] [Batch 6/150] [D loss: 0.224886] [G loss: 215.069244] time: 0:14:13.558098\n",
            "[Epoch 4/5] [Batch 7/150] [D loss: 0.239779] [G loss: 173.888779] time: 0:14:14.888945\n",
            "[Epoch 4/5] [Batch 8/150] [D loss: 0.240423] [G loss: 264.299316] time: 0:14:16.220055\n",
            "[Epoch 4/5] [Batch 9/150] [D loss: 0.225646] [G loss: 278.805664] time: 0:14:17.555797\n",
            "[Epoch 4/5] [Batch 10/150] [D loss: 0.231828] [G loss: 346.405518] time: 0:14:18.896102\n",
            "[Epoch 4/5] [Batch 11/150] [D loss: 0.233336] [G loss: 346.503571] time: 0:14:20.250764\n",
            "[Epoch 4/5] [Batch 12/150] [D loss: 0.238810] [G loss: 509.406830] time: 0:14:21.599972\n",
            "[Epoch 4/5] [Batch 13/150] [D loss: 0.226511] [G loss: 191.184998] time: 0:14:22.928179\n",
            "[Epoch 4/5] [Batch 14/150] [D loss: 0.233755] [G loss: 244.489456] time: 0:14:24.258372\n",
            "[Epoch 4/5] [Batch 15/150] [D loss: 0.227246] [G loss: 226.653793] time: 0:14:25.715397\n",
            "[Epoch 4/5] [Batch 16/150] [D loss: 0.235711] [G loss: 281.465973] time: 0:14:27.040393\n",
            "[Epoch 4/5] [Batch 17/150] [D loss: 0.236608] [G loss: 197.640030] time: 0:14:28.344812\n",
            "[Epoch 4/5] [Batch 18/150] [D loss: 0.240952] [G loss: 205.061829] time: 0:14:29.683261\n",
            "[Epoch 4/5] [Batch 19/150] [D loss: 0.230220] [G loss: 183.517853] time: 0:14:31.020424\n",
            "[Epoch 4/5] [Batch 20/150] [D loss: 0.230268] [G loss: 359.675720] time: 0:14:32.359890\n",
            "[Epoch 4/5] [Batch 21/150] [D loss: 0.239740] [G loss: 215.292572] time: 0:14:33.697452\n",
            "[Epoch 4/5] [Batch 22/150] [D loss: 0.220652] [G loss: 268.315399] time: 0:14:35.042101\n",
            "[Epoch 4/5] [Batch 23/150] [D loss: 0.245708] [G loss: 286.770294] time: 0:14:36.385461\n",
            "[Epoch 4/5] [Batch 24/150] [D loss: 0.232339] [G loss: 273.140869] time: 0:14:37.735834\n",
            "[Epoch 4/5] [Batch 25/150] [D loss: 0.234072] [G loss: 218.653610] time: 0:14:39.088720\n",
            "[Epoch 4/5] [Batch 26/150] [D loss: 0.236615] [G loss: 245.103348] time: 0:14:41.319128\n",
            "[Epoch 4/5] [Batch 27/150] [D loss: 0.238381] [G loss: 251.706009] time: 0:14:42.647699\n",
            "[Epoch 4/5] [Batch 28/150] [D loss: 0.233847] [G loss: 229.288010] time: 0:14:44.004142\n",
            "[Epoch 4/5] [Batch 29/150] [D loss: 0.234908] [G loss: 152.664322] time: 0:14:45.353261\n",
            "[Epoch 4/5] [Batch 30/150] [D loss: 0.249885] [G loss: 190.840408] time: 0:14:46.680981\n",
            "[Epoch 4/5] [Batch 31/150] [D loss: 0.238584] [G loss: 160.779449] time: 0:14:48.045796\n",
            "[Epoch 4/5] [Batch 32/150] [D loss: 0.228798] [G loss: 304.710022] time: 0:14:49.389359\n",
            "[Epoch 4/5] [Batch 33/150] [D loss: 0.242247] [G loss: 295.702148] time: 0:14:50.733640\n",
            "[Epoch 4/5] [Batch 34/150] [D loss: 0.223584] [G loss: 346.522949] time: 0:14:52.085224\n",
            "[Epoch 4/5] [Batch 35/150] [D loss: 0.225220] [G loss: 250.497894] time: 0:14:53.432087\n",
            "[Epoch 4/5] [Batch 36/150] [D loss: 0.240548] [G loss: 382.721100] time: 0:14:54.777840\n",
            "[Epoch 4/5] [Batch 37/150] [D loss: 0.234983] [G loss: 401.222198] time: 0:14:56.117520\n",
            "[Epoch 4/5] [Batch 38/150] [D loss: 0.225294] [G loss: 282.781921] time: 0:14:57.454881\n",
            "[Epoch 4/5] [Batch 39/150] [D loss: 0.233400] [G loss: 288.605347] time: 0:14:58.771181\n",
            "[Epoch 4/5] [Batch 40/150] [D loss: 0.224724] [G loss: 292.547211] time: 0:15:00.254251\n",
            "[Epoch 4/5] [Batch 41/150] [D loss: 0.231646] [G loss: 312.517151] time: 0:15:01.598132\n",
            "[Epoch 4/5] [Batch 42/150] [D loss: 0.231126] [G loss: 178.980103] time: 0:15:02.928536\n",
            "[Epoch 4/5] [Batch 43/150] [D loss: 0.224392] [G loss: 312.239441] time: 0:15:04.270840\n",
            "[Epoch 4/5] [Batch 44/150] [D loss: 0.233361] [G loss: 359.139038] time: 0:15:05.646056\n",
            "[Epoch 4/5] [Batch 45/150] [D loss: 0.242884] [G loss: 421.554596] time: 0:15:06.981829\n",
            "[Epoch 4/5] [Batch 46/150] [D loss: 0.237284] [G loss: 343.562866] time: 0:15:08.317613\n",
            "[Epoch 4/5] [Batch 47/150] [D loss: 0.270219] [G loss: 1008.960205] time: 0:15:09.637773\n",
            "[Epoch 4/5] [Batch 48/150] [D loss: 0.229819] [G loss: 262.378448] time: 0:15:10.963518\n",
            "[Epoch 4/5] [Batch 49/150] [D loss: 0.241971] [G loss: 366.669525] time: 0:15:12.309987\n",
            "[Epoch 4/5] [Batch 50/150] [D loss: 0.224758] [G loss: 333.521332] time: 0:15:13.635216\n",
            "[Epoch 4/5] [Batch 51/150] [D loss: 0.238956] [G loss: 195.499557] time: 0:15:15.845134\n",
            "[Epoch 4/5] [Batch 52/150] [D loss: 0.229692] [G loss: 347.354401] time: 0:15:17.170969\n",
            "[Epoch 4/5] [Batch 53/150] [D loss: 0.245945] [G loss: 295.831879] time: 0:15:18.485924\n",
            "[Epoch 4/5] [Batch 54/150] [D loss: 0.233242] [G loss: 259.246613] time: 0:15:19.810663\n",
            "[Epoch 4/5] [Batch 55/150] [D loss: 0.237096] [G loss: 465.512756] time: 0:15:21.202624\n",
            "[Epoch 4/5] [Batch 56/150] [D loss: 0.229931] [G loss: 149.867508] time: 0:15:22.550841\n",
            "[Epoch 4/5] [Batch 57/150] [D loss: 0.229189] [G loss: 343.730347] time: 0:15:24.090368\n",
            "[Epoch 4/5] [Batch 58/150] [D loss: 0.233459] [G loss: 219.184631] time: 0:15:25.423022\n",
            "[Epoch 4/5] [Batch 59/150] [D loss: 0.221361] [G loss: 280.547028] time: 0:15:26.765494\n",
            "[Epoch 4/5] [Batch 60/150] [D loss: 0.227898] [G loss: 262.548950] time: 0:15:28.090342\n",
            "[Epoch 4/5] [Batch 61/150] [D loss: 0.230487] [G loss: 382.937286] time: 0:15:29.423380\n",
            "[Epoch 4/5] [Batch 62/150] [D loss: 0.223181] [G loss: 344.394073] time: 0:15:30.764091\n",
            "[Epoch 4/5] [Batch 63/150] [D loss: 0.233003] [G loss: 341.290344] time: 0:15:32.096149\n",
            "[Epoch 4/5] [Batch 64/150] [D loss: 0.232322] [G loss: 169.637268] time: 0:15:33.415406\n",
            "[Epoch 4/5] [Batch 65/150] [D loss: 0.234271] [G loss: 292.870148] time: 0:15:34.881956\n",
            "[Epoch 4/5] [Batch 66/150] [D loss: 0.241730] [G loss: 338.997589] time: 0:15:36.210092\n",
            "[Epoch 4/5] [Batch 67/150] [D loss: 0.242562] [G loss: 440.771881] time: 0:15:37.549293\n",
            "[Epoch 4/5] [Batch 68/150] [D loss: 0.237273] [G loss: 312.775604] time: 0:15:38.889669\n",
            "[Epoch 4/5] [Batch 69/150] [D loss: 0.240662] [G loss: 166.328964] time: 0:15:40.215733\n",
            "[Epoch 4/5] [Batch 70/150] [D loss: 0.235585] [G loss: 397.515198] time: 0:15:41.548185\n",
            "[Epoch 4/5] [Batch 71/150] [D loss: 0.233400] [G loss: 222.633514] time: 0:15:42.878620\n",
            "[Epoch 4/5] [Batch 72/150] [D loss: 0.235414] [G loss: 444.047485] time: 0:15:44.260863\n",
            "[Epoch 4/5] [Batch 73/150] [D loss: 0.231292] [G loss: 168.078537] time: 0:15:45.596875\n",
            "[Epoch 4/5] [Batch 74/150] [D loss: 0.240595] [G loss: 203.888657] time: 0:15:46.925314\n",
            "[Epoch 4/5] [Batch 75/150] [D loss: 0.224168] [G loss: 361.393463] time: 0:15:48.251758\n",
            "[Epoch 4/5] [Batch 76/150] [D loss: 0.238431] [G loss: 177.787170] time: 0:15:50.440637\n",
            "[Epoch 4/5] [Batch 77/150] [D loss: 0.281614] [G loss: 131.346451] time: 0:15:51.788452\n",
            "[Epoch 4/5] [Batch 78/150] [D loss: 0.230200] [G loss: 312.064331] time: 0:15:53.121712\n",
            "[Epoch 4/5] [Batch 79/150] [D loss: 0.238521] [G loss: 315.586395] time: 0:15:54.452678\n",
            "[Epoch 4/5] [Batch 80/150] [D loss: 0.225466] [G loss: 276.443115] time: 0:15:55.802348\n",
            "[Epoch 4/5] [Batch 81/150] [D loss: 0.233810] [G loss: 200.061203] time: 0:15:57.147429\n",
            "[Epoch 4/5] [Batch 82/150] [D loss: 0.231122] [G loss: 299.352264] time: 0:15:58.469879\n",
            "[Epoch 4/5] [Batch 83/150] [D loss: 0.230602] [G loss: 281.391724] time: 0:15:59.796279\n",
            "[Epoch 4/5] [Batch 84/150] [D loss: 0.225255] [G loss: 260.963348] time: 0:16:01.129777\n",
            "[Epoch 4/5] [Batch 85/150] [D loss: 0.236716] [G loss: 248.001282] time: 0:16:02.427152\n",
            "[Epoch 4/5] [Batch 86/150] [D loss: 0.225700] [G loss: 193.327866] time: 0:16:03.771655\n",
            "[Epoch 4/5] [Batch 87/150] [D loss: 0.231158] [G loss: 282.577576] time: 0:16:05.121788\n",
            "[Epoch 4/5] [Batch 88/150] [D loss: 0.236390] [G loss: 129.150574] time: 0:16:06.454667\n",
            "[Epoch 4/5] [Batch 89/150] [D loss: 0.230083] [G loss: 170.534103] time: 0:16:07.764538\n",
            "[Epoch 4/5] [Batch 90/150] [D loss: 0.245017] [G loss: 215.708557] time: 0:16:09.234924\n",
            "[Epoch 4/5] [Batch 91/150] [D loss: 0.223548] [G loss: 237.760880] time: 0:16:10.548633\n",
            "[Epoch 4/5] [Batch 92/150] [D loss: 0.238605] [G loss: 175.589798] time: 0:16:11.867543\n",
            "[Epoch 4/5] [Batch 93/150] [D loss: 0.230034] [G loss: 298.640533] time: 0:16:13.217124\n",
            "[Epoch 4/5] [Batch 94/150] [D loss: 0.233062] [G loss: 359.345886] time: 0:16:14.562818\n",
            "[Epoch 4/5] [Batch 95/150] [D loss: 0.229569] [G loss: 147.066757] time: 0:16:15.891245\n",
            "[Epoch 4/5] [Batch 96/150] [D loss: 0.244469] [G loss: 213.096695] time: 0:16:17.212668\n",
            "[Epoch 4/5] [Batch 97/150] [D loss: 0.235130] [G loss: 387.793152] time: 0:16:18.539528\n",
            "[Epoch 4/5] [Batch 98/150] [D loss: 0.215261] [G loss: 300.845398] time: 0:16:19.871718\n",
            "[Epoch 4/5] [Batch 99/150] [D loss: 0.228505] [G loss: 281.695068] time: 0:16:21.207743\n",
            "[Epoch 4/5] [Batch 100/150] [D loss: 0.229412] [G loss: 230.737183] time: 0:16:22.530186\n",
            "[Epoch 4/5] [Batch 101/150] [D loss: 0.241443] [G loss: 355.706726] time: 0:16:24.800603\n",
            "[Epoch 4/5] [Batch 102/150] [D loss: 0.235078] [G loss: 235.559097] time: 0:16:26.117492\n",
            "[Epoch 4/5] [Batch 103/150] [D loss: 0.248381] [G loss: 246.398743] time: 0:16:27.436886\n",
            "[Epoch 4/5] [Batch 104/150] [D loss: 0.232838] [G loss: 248.212631] time: 0:16:28.778660\n",
            "[Epoch 4/5] [Batch 105/150] [D loss: 0.245159] [G loss: 368.702148] time: 0:16:30.107847\n",
            "[Epoch 4/5] [Batch 106/150] [D loss: 0.238324] [G loss: 205.906906] time: 0:16:31.431137\n",
            "[Epoch 4/5] [Batch 107/150] [D loss: 0.225313] [G loss: 393.689423] time: 0:16:32.748295\n",
            "[Epoch 4/5] [Batch 108/150] [D loss: 0.228667] [G loss: 183.867859] time: 0:16:34.071530\n",
            "[Epoch 4/5] [Batch 109/150] [D loss: 0.221056] [G loss: 253.404037] time: 0:16:35.415400\n",
            "[Epoch 4/5] [Batch 110/150] [D loss: 0.233410] [G loss: 414.361237] time: 0:16:36.746867\n",
            "[Epoch 4/5] [Batch 111/150] [D loss: 0.230503] [G loss: 349.216248] time: 0:16:38.069236\n",
            "[Epoch 4/5] [Batch 112/150] [D loss: 0.258707] [G loss: 75.926567] time: 0:16:39.382968\n",
            "[Epoch 4/5] [Batch 113/150] [D loss: 0.222037] [G loss: 319.301910] time: 0:16:40.704479\n",
            "[Epoch 4/5] [Batch 114/150] [D loss: 0.233527] [G loss: 431.441101] time: 0:16:42.040184\n",
            "[Epoch 4/5] [Batch 115/150] [D loss: 0.229341] [G loss: 180.386765] time: 0:16:43.527863\n",
            "[Epoch 4/5] [Batch 116/150] [D loss: 0.228597] [G loss: 232.610352] time: 0:16:44.881328\n",
            "[Epoch 4/5] [Batch 117/150] [D loss: 0.239090] [G loss: 166.620880] time: 0:16:46.217120\n",
            "[Epoch 4/5] [Batch 118/150] [D loss: 0.235707] [G loss: 158.234741] time: 0:16:47.556423\n",
            "[Epoch 4/5] [Batch 119/150] [D loss: 0.244309] [G loss: 127.268135] time: 0:16:48.886899\n",
            "[Epoch 4/5] [Batch 120/150] [D loss: 0.227972] [G loss: 336.657990] time: 0:16:50.211979\n",
            "[Epoch 4/5] [Batch 121/150] [D loss: 0.226584] [G loss: 282.024200] time: 0:16:51.555435\n",
            "[Epoch 4/5] [Batch 122/150] [D loss: 0.230703] [G loss: 186.976059] time: 0:16:52.885243\n",
            "[Epoch 4/5] [Batch 123/150] [D loss: 0.232829] [G loss: 280.372101] time: 0:16:54.225041\n",
            "[Epoch 4/5] [Batch 124/150] [D loss: 0.225014] [G loss: 278.588623] time: 0:16:55.551877\n",
            "[Epoch 4/5] [Batch 125/150] [D loss: 0.229205] [G loss: 120.344910] time: 0:16:56.879656\n",
            "[Epoch 4/5] [Batch 126/150] [D loss: 0.235705] [G loss: 155.969208] time: 0:16:59.164623\n",
            "[Epoch 4/5] [Batch 127/150] [D loss: 0.241056] [G loss: 424.841675] time: 0:17:00.524457\n",
            "[Epoch 4/5] [Batch 128/150] [D loss: 0.218916] [G loss: 192.490616] time: 0:17:01.846969\n",
            "[Epoch 4/5] [Batch 129/150] [D loss: 0.237570] [G loss: 206.974701] time: 0:17:03.706150\n",
            "[Epoch 4/5] [Batch 130/150] [D loss: 0.231872] [G loss: 346.465637] time: 0:17:05.043644\n",
            "[Epoch 4/5] [Batch 131/150] [D loss: 0.234850] [G loss: 217.041290] time: 0:17:06.384852\n",
            "[Epoch 4/5] [Batch 132/150] [D loss: 0.238888] [G loss: 346.064026] time: 0:17:07.703581\n",
            "[Epoch 4/5] [Batch 133/150] [D loss: 0.230165] [G loss: 247.254944] time: 0:17:09.044381\n",
            "[Epoch 4/5] [Batch 134/150] [D loss: 0.231939] [G loss: 269.620148] time: 0:17:10.375662\n",
            "[Epoch 4/5] [Batch 135/150] [D loss: 0.229331] [G loss: 276.022339] time: 0:17:11.688603\n",
            "[Epoch 4/5] [Batch 136/150] [D loss: 0.224719] [G loss: 344.565033] time: 0:17:13.039392\n",
            "[Epoch 4/5] [Batch 137/150] [D loss: 0.231133] [G loss: 257.725403] time: 0:17:14.382685\n",
            "[Epoch 4/5] [Batch 138/150] [D loss: 0.237098] [G loss: 484.975739] time: 0:17:15.712495\n",
            "[Epoch 4/5] [Batch 139/150] [D loss: 0.226588] [G loss: 162.018280] time: 0:17:17.039773\n",
            "[Epoch 4/5] [Batch 140/150] [D loss: 0.231726] [G loss: 223.732956] time: 0:17:18.500947\n",
            "[Epoch 4/5] [Batch 141/150] [D loss: 0.229789] [G loss: 247.201950] time: 0:17:20.620929\n",
            "[Epoch 4/5] [Batch 142/150] [D loss: 0.225942] [G loss: 214.738922] time: 0:17:21.937549\n",
            "[Epoch 4/5] [Batch 143/150] [D loss: 0.228679] [G loss: 226.059448] time: 0:17:23.270160\n",
            "[Epoch 4/5] [Batch 144/150] [D loss: 0.222766] [G loss: 250.166046] time: 0:17:24.609831\n",
            "[Epoch 4/5] [Batch 145/150] [D loss: 0.237386] [G loss: 324.507507] time: 0:17:26.025801\n",
            "[Epoch 4/5] [Batch 146/150] [D loss: 0.237746] [G loss: 240.864899] time: 0:17:27.354619\n",
            "[Epoch 4/5] [Batch 147/150] [D loss: 0.223332] [G loss: 195.725189] time: 0:17:28.667681\n",
            "[Epoch 4/5] [Batch 148/150] [D loss: 0.237986] [G loss: 315.993073] time: 0:17:29.980808\n",
            "Traceback (most recent call last):\n",
            "  File \"ID-CGAN.py\", line 315, in <module>\n",
            "    gan.train(epochs=5, batch_size=1, sample_interval=25)\n",
            "  File \"ID-CGAN.py\", line 279, in train\n",
            "    self.combined.save_weights(\"./saved_models/com_model.h5\")\n",
            "AttributeError: 'IDGAN' object has no attribute 'combined'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}