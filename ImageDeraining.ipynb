{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageDeraining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1frXGdUERrMswNBwodIIloJErST5JHh_d",
      "authorship_tag": "ABX9TyOCcx38g/UmPWDxnOjTsvLH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pallabjyotidk/ImageDerainingProject/blob/main/ImageDeraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKvUIKCzhBLw",
        "outputId": "760f0f9d-b944-40c9-fd22-6d61cc49b9a2"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUD8awmcvZ6O",
        "outputId": "f36ade6b-12e7-4e2b-9c2f-4d646099ab99"
      },
      "source": [
        "cd drive/MyDrive/Project/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXtjR0Nj4lU8",
        "outputId": "0b349d3d-eec0-4579-be82-fbe36724a4c3"
      },
      "source": [
        "pip install scipy==1.2.0"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy==1.2.0 in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.2.0) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVnonAXRvM-z",
        "outputId": "822ae8d3-fd89-4d12-dab2-1c9b7ba158a8"
      },
      "source": [
        "!python ID-CGAN.py"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-08 12:44:43.901404: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-04-08 12:44:45.394017: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-04-08 12:44:45.395009: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-04-08 12:44:45.406034: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-04-08 12:44:45.406088: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (85ba3f981226): /proc/driver/nvidia/version does not exist\n",
            "2021-04-08 12:44:45.406315: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-04-08 12:44:45.406467: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3504: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
            "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n",
            "2021-04-08 12:44:47.546936: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2021-04-08 12:44:47.547384: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 1999995000 Hz\n",
            "[Epoch 0/1] [Batch 0/150] [D loss: 0.311245] [G loss: 1552.435059] time: 0:00:15.037691\n",
            "[Epoch 0/1] [Batch 1/150] [D loss: 0.307022] [G loss: 925.258057] time: 0:00:33.855681\n",
            "[Epoch 0/1] [Batch 2/150] [D loss: 0.310052] [G loss: 735.986267] time: 0:00:48.067410\n",
            "[Epoch 0/1] [Batch 3/150] [D loss: 0.305134] [G loss: 572.758728] time: 0:01:02.290193\n",
            "[Epoch 0/1] [Batch 4/150] [D loss: 0.301945] [G loss: 842.380554] time: 0:01:16.629980\n",
            "[Epoch 0/1] [Batch 5/150] [D loss: 0.302099] [G loss: 585.659546] time: 0:01:30.922729\n",
            "[Epoch 0/1] [Batch 6/150] [D loss: 0.304834] [G loss: 503.353485] time: 0:01:45.261689\n",
            "[Epoch 0/1] [Batch 7/150] [D loss: 0.302738] [G loss: 422.109283] time: 0:01:59.521735\n",
            "[Epoch 0/1] [Batch 8/150] [D loss: 0.299398] [G loss: 496.484680] time: 0:02:13.789430\n",
            "[Epoch 0/1] [Batch 9/150] [D loss: 0.307601] [G loss: 517.405884] time: 0:02:28.088856\n",
            "[Epoch 0/1] [Batch 10/150] [D loss: 0.299739] [G loss: 602.169617] time: 0:02:42.372460\n",
            "[Epoch 0/1] [Batch 11/150] [D loss: 0.304059] [G loss: 680.854126] time: 0:02:56.670708\n",
            "[Epoch 0/1] [Batch 12/150] [D loss: 0.309993] [G loss: 921.160278] time: 0:03:10.993535\n",
            "[Epoch 0/1] [Batch 13/150] [D loss: 0.301600] [G loss: 373.657806] time: 0:03:25.260782\n",
            "[Epoch 0/1] [Batch 14/150] [D loss: 0.303350] [G loss: 454.800934] time: 0:03:39.540644\n",
            "[Epoch 0/1] [Batch 15/150] [D loss: 0.297565] [G loss: 344.765533] time: 0:03:53.709889\n",
            "[Epoch 0/1] [Batch 16/150] [D loss: 0.299356] [G loss: 460.900146] time: 0:04:07.851312\n",
            "[Epoch 0/1] [Batch 17/150] [D loss: 0.297392] [G loss: 403.943390] time: 0:04:22.043873\n",
            "[Epoch 0/1] [Batch 18/150] [D loss: 0.295815] [G loss: 280.085541] time: 0:04:36.307231\n",
            "[Epoch 0/1] [Batch 19/150] [D loss: 0.300965] [G loss: 340.990204] time: 0:04:50.612643\n",
            "[Epoch 0/1] [Batch 20/150] [D loss: 0.295169] [G loss: 623.917847] time: 0:05:04.936307\n",
            "[Epoch 0/1] [Batch 21/150] [D loss: 0.300925] [G loss: 389.747406] time: 0:05:19.219407\n",
            "[Epoch 0/1] [Batch 22/150] [D loss: 0.296584] [G loss: 465.093994] time: 0:05:33.545799\n",
            "[Epoch 0/1] [Batch 23/150] [D loss: 0.302766] [G loss: 552.008728] time: 0:05:47.923287\n",
            "[Epoch 0/1] [Batch 24/150] [D loss: 0.299218] [G loss: 460.256470] time: 0:06:02.250836\n",
            "[Epoch 0/1] [Batch 25/150] [D loss: 0.295161] [G loss: 445.380981] time: 0:06:16.569534\n",
            "[Epoch 0/1] [Batch 26/150] [D loss: 0.301425] [G loss: 427.398529] time: 0:06:36.068158\n",
            "[Epoch 0/1] [Batch 27/150] [D loss: 0.300644] [G loss: 421.263306] time: 0:06:50.329917\n",
            "[Epoch 0/1] [Batch 28/150] [D loss: 0.305889] [G loss: 319.675049] time: 0:07:04.614093\n",
            "[Epoch 0/1] [Batch 29/150] [D loss: 0.296029] [G loss: 326.299408] time: 0:07:18.882730\n",
            "[Epoch 0/1] [Batch 30/150] [D loss: 0.303655] [G loss: 398.515381] time: 0:07:33.139171\n",
            "[Epoch 0/1] [Batch 31/150] [D loss: 0.293031] [G loss: 315.426453] time: 0:07:47.448802\n",
            "[Epoch 0/1] [Batch 32/150] [D loss: 0.300705] [G loss: 481.457001] time: 0:08:01.698138\n",
            "[Epoch 0/1] [Batch 33/150] [D loss: 0.310006] [G loss: 461.686951] time: 0:08:15.942297\n",
            "[Epoch 0/1] [Batch 34/150] [D loss: 0.302585] [G loss: 577.926086] time: 0:08:30.228636\n",
            "[Epoch 0/1] [Batch 35/150] [D loss: 0.296857] [G loss: 448.016296] time: 0:08:44.527022\n",
            "[Epoch 0/1] [Batch 36/150] [D loss: 0.301582] [G loss: 642.146545] time: 0:08:58.807793\n",
            "[Epoch 0/1] [Batch 37/150] [D loss: 0.302480] [G loss: 731.964478] time: 0:09:13.080937\n",
            "[Epoch 0/1] [Batch 38/150] [D loss: 0.297228] [G loss: 400.408691] time: 0:09:27.341236\n",
            "[Epoch 0/1] [Batch 39/150] [D loss: 0.296603] [G loss: 526.849792] time: 0:09:41.597642\n",
            "[Epoch 0/1] [Batch 40/150] [D loss: 0.296724] [G loss: 500.240479] time: 0:09:55.936584\n",
            "[Epoch 0/1] [Batch 41/150] [D loss: 0.304597] [G loss: 507.792694] time: 0:10:10.217789\n",
            "[Epoch 0/1] [Batch 42/150] [D loss: 0.297542] [G loss: 302.732147] time: 0:10:24.484661\n",
            "[Epoch 0/1] [Batch 43/150] [D loss: 0.297285] [G loss: 503.208557] time: 0:10:38.754256\n",
            "[Epoch 0/1] [Batch 44/150] [D loss: 0.301138] [G loss: 558.532959] time: 0:10:53.179410\n",
            "[Epoch 0/1] [Batch 45/150] [D loss: 0.297864] [G loss: 670.702271] time: 0:11:07.460075\n",
            "[Epoch 0/1] [Batch 46/150] [D loss: 0.296199] [G loss: 591.316772] time: 0:11:21.801320\n",
            "[Epoch 0/1] [Batch 47/150] [D loss: 0.310038] [G loss: 991.731934] time: 0:11:36.009723\n",
            "[Epoch 0/1] [Batch 48/150] [D loss: 0.292480] [G loss: 451.309387] time: 0:11:50.297126\n",
            "[Epoch 0/1] [Batch 49/150] [D loss: 0.297719] [G loss: 547.866333] time: 0:12:04.564147\n",
            "[Epoch 0/1] [Batch 50/150] [D loss: 0.297082] [G loss: 486.674530] time: 0:12:18.854138\n",
            "[Epoch 0/1] [Batch 51/150] [D loss: 0.291249] [G loss: 243.853470] time: 0:12:37.743024\n",
            "[Epoch 0/1] [Batch 52/150] [D loss: 0.295877] [G loss: 579.748657] time: 0:12:52.037199\n",
            "[Epoch 0/1] [Batch 53/150] [D loss: 0.298155] [G loss: 464.457031] time: 0:13:06.354765\n",
            "[Epoch 0/1] [Batch 54/150] [D loss: 0.292784] [G loss: 398.269073] time: 0:13:20.592544\n",
            "[Epoch 0/1] [Batch 55/150] [D loss: 0.296514] [G loss: 703.154358] time: 0:13:34.929378\n",
            "[Epoch 0/1] [Batch 56/150] [D loss: 0.292214] [G loss: 246.945389] time: 0:13:49.221133\n",
            "[Epoch 0/1] [Batch 57/150] [D loss: 0.295379] [G loss: 534.508057] time: 0:14:03.736315\n",
            "[Epoch 0/1] [Batch 58/150] [D loss: 0.297461] [G loss: 353.873169] time: 0:14:18.008743\n",
            "[Epoch 0/1] [Batch 59/150] [D loss: 0.292596] [G loss: 421.139893] time: 0:14:32.335961\n",
            "[Epoch 0/1] [Batch 60/150] [D loss: 0.294513] [G loss: 419.188873] time: 0:14:46.665194\n",
            "[Epoch 0/1] [Batch 61/150] [D loss: 0.294616] [G loss: 545.713562] time: 0:15:00.999983\n",
            "[Epoch 0/1] [Batch 62/150] [D loss: 0.297297] [G loss: 422.477264] time: 0:15:15.292085\n",
            "[Epoch 0/1] [Batch 63/150] [D loss: 0.296729] [G loss: 510.329559] time: 0:15:29.541457\n",
            "[Epoch 0/1] [Batch 64/150] [D loss: 0.290221] [G loss: 270.107391] time: 0:15:43.840319\n",
            "[Epoch 0/1] [Batch 65/150] [D loss: 0.297315] [G loss: 470.425415] time: 0:15:58.171060\n",
            "[Epoch 0/1] [Batch 66/150] [D loss: 0.301648] [G loss: 533.409241] time: 0:16:12.219369\n",
            "[Epoch 0/1] [Batch 67/150] [D loss: 0.298463] [G loss: 689.425842] time: 0:16:26.256631\n",
            "[Epoch 0/1] [Batch 68/150] [D loss: 0.295925] [G loss: 455.451294] time: 0:16:40.670368\n",
            "[Epoch 0/1] [Batch 69/150] [D loss: 0.295778] [G loss: 225.443634] time: 0:16:55.025497\n",
            "[Epoch 0/1] [Batch 70/150] [D loss: 0.294956] [G loss: 602.573914] time: 0:17:09.348451\n",
            "[Epoch 0/1] [Batch 71/150] [D loss: 0.293458] [G loss: 306.926941] time: 0:17:23.716080\n",
            "[Epoch 0/1] [Batch 72/150] [D loss: 0.293997] [G loss: 693.893860] time: 0:17:38.209412\n",
            "[Epoch 0/1] [Batch 73/150] [D loss: 0.289087] [G loss: 304.536591] time: 0:17:52.578232\n",
            "[Epoch 0/1] [Batch 74/150] [D loss: 0.301786] [G loss: 386.967957] time: 0:18:06.958429\n",
            "[Epoch 0/1] [Batch 75/150] [D loss: 0.291317] [G loss: 535.878296] time: 0:18:21.425972\n",
            "[Epoch 0/1] [Batch 76/150] [D loss: 0.296349] [G loss: 358.079163] time: 0:18:40.318647\n",
            "[Epoch 0/1] [Batch 77/150] [D loss: 0.299568] [G loss: 193.994492] time: 0:18:54.580075\n",
            "[Epoch 0/1] [Batch 78/150] [D loss: 0.287499] [G loss: 451.630951] time: 0:19:08.966576\n",
            "[Epoch 0/1] [Batch 79/150] [D loss: 0.301645] [G loss: 521.975525] time: 0:19:23.240762\n",
            "[Epoch 0/1] [Batch 80/150] [D loss: 0.285742] [G loss: 418.125305] time: 0:19:37.520722\n",
            "[Epoch 0/1] [Batch 81/150] [D loss: 0.294461] [G loss: 358.788391] time: 0:19:51.820168\n",
            "[Epoch 0/1] [Batch 82/150] [D loss: 0.298378] [G loss: 464.799713] time: 0:20:06.128300\n",
            "[Epoch 0/1] [Batch 83/150] [D loss: 0.298827] [G loss: 448.980133] time: 0:20:20.443821\n",
            "[Epoch 0/1] [Batch 84/150] [D loss: 0.287104] [G loss: 389.460327] time: 0:20:34.781947\n",
            "[Epoch 0/1] [Batch 85/150] [D loss: 0.293047] [G loss: 338.493744] time: 0:20:49.099277\n",
            "[Epoch 0/1] [Batch 86/150] [D loss: 0.297436] [G loss: 284.603027] time: 0:21:03.292386\n",
            "[Epoch 0/1] [Batch 87/150] [D loss: 0.294402] [G loss: 448.311707] time: 0:21:17.583200\n",
            "[Epoch 0/1] [Batch 88/150] [D loss: 0.290896] [G loss: 186.343048] time: 0:21:31.923874\n",
            "[Epoch 0/1] [Batch 89/150] [D loss: 0.294756] [G loss: 253.304138] time: 0:21:46.215256\n",
            "[Epoch 0/1] [Batch 90/150] [D loss: 0.288945] [G loss: 306.527496] time: 0:22:00.413464\n",
            "[Epoch 0/1] [Batch 91/150] [D loss: 0.295521] [G loss: 348.441101] time: 0:22:14.322146\n",
            "[Epoch 0/1] [Batch 92/150] [D loss: 0.288375] [G loss: 233.351303] time: 0:22:28.193136\n",
            "[Epoch 0/1] [Batch 93/150] [D loss: 0.290561] [G loss: 437.002747] time: 0:22:42.449304\n",
            "[Epoch 0/1] [Batch 94/150] [D loss: 0.293383] [G loss: 506.428223] time: 0:22:56.651612\n",
            "[Epoch 0/1] [Batch 95/150] [D loss: 0.288815] [G loss: 234.865524] time: 0:23:10.884248\n",
            "[Epoch 0/1] [Batch 96/150] [D loss: 0.295572] [G loss: 318.910736] time: 0:23:25.157351\n",
            "[Epoch 0/1] [Batch 97/150] [D loss: 0.298399] [G loss: 532.656372] time: 0:23:39.533869\n",
            "[Epoch 0/1] [Batch 98/150] [D loss: 0.281886] [G loss: 417.372223] time: 0:23:53.964088\n",
            "[Epoch 0/1] [Batch 99/150] [D loss: 0.285269] [G loss: 420.811005] time: 0:24:08.379046\n",
            "[Epoch 0/1] [Batch 100/150] [D loss: 0.282308] [G loss: 285.455780] time: 0:24:22.826523\n",
            "[Epoch 0/1] [Batch 101/150] [D loss: 0.296317] [G loss: 519.762024] time: 0:24:41.926994\n",
            "[Epoch 0/1] [Batch 102/150] [D loss: 0.288195] [G loss: 353.125153] time: 0:24:56.187130\n",
            "[Epoch 0/1] [Batch 103/150] [D loss: 0.294320] [G loss: 410.913391] time: 0:25:10.437857\n",
            "[Epoch 0/1] [Batch 104/150] [D loss: 0.293329] [G loss: 399.992950] time: 0:25:24.704663\n",
            "[Epoch 0/1] [Batch 105/150] [D loss: 0.299633] [G loss: 547.079529] time: 0:25:39.018946\n",
            "[Epoch 0/1] [Batch 106/150] [D loss: 0.286499] [G loss: 232.786240] time: 0:25:53.227532\n",
            "[Epoch 0/1] [Batch 107/150] [D loss: 0.287378] [G loss: 564.915344] time: 0:26:07.559603\n",
            "[Epoch 0/1] [Batch 108/150] [D loss: 0.285656] [G loss: 249.110428] time: 0:26:21.897816\n",
            "[Epoch 0/1] [Batch 109/150] [D loss: 0.286284] [G loss: 383.302887] time: 0:26:36.202170\n",
            "[Epoch 0/1] [Batch 110/150] [D loss: 0.289535] [G loss: 523.335449] time: 0:26:50.550483\n",
            "[Epoch 0/1] [Batch 111/150] [D loss: 0.288237] [G loss: 491.359802] time: 0:27:04.879848\n",
            "[Epoch 0/1] [Batch 112/150] [D loss: 0.294739] [G loss: 127.773193] time: 0:27:19.190966\n",
            "[Epoch 0/1] [Batch 113/150] [D loss: 0.286109] [G loss: 447.681396] time: 0:27:33.507908\n",
            "[Epoch 0/1] [Batch 114/150] [D loss: 0.288222] [G loss: 584.996948] time: 0:27:47.830114\n",
            "[Epoch 0/1] [Batch 115/150] [D loss: 0.281075] [G loss: 296.259583] time: 0:28:02.152321\n",
            "[Epoch 0/1] [Batch 116/150] [D loss: 0.284810] [G loss: 347.083893] time: 0:28:16.422001\n",
            "[Epoch 0/1] [Batch 117/150] [D loss: 0.290179] [G loss: 217.298294] time: 0:28:30.701999\n",
            "[Epoch 0/1] [Batch 118/150] [D loss: 0.284778] [G loss: 203.247253] time: 0:28:45.130327\n",
            "[Epoch 0/1] [Batch 119/150] [D loss: 0.287268] [G loss: 193.805176] time: 0:28:59.473136\n",
            "[Epoch 0/1] [Batch 120/150] [D loss: 0.282043] [G loss: 504.331421] time: 0:29:13.842365\n",
            "[Epoch 0/1] [Batch 121/150] [D loss: 0.285931] [G loss: 393.804993] time: 0:29:28.270673\n",
            "[Epoch 0/1] [Batch 122/150] [D loss: 0.287220] [G loss: 272.256042] time: 0:29:42.672375\n",
            "[Epoch 0/1] [Batch 123/150] [D loss: 0.284918] [G loss: 389.761841] time: 0:29:57.051068\n",
            "[Epoch 0/1] [Batch 124/150] [D loss: 0.289244] [G loss: 403.136139] time: 0:30:11.361604\n",
            "[Epoch 0/1] [Batch 125/150] [D loss: 0.287338] [G loss: 162.518555] time: 0:30:25.712654\n",
            "[Epoch 0/1] [Batch 126/150] [D loss: 0.286443] [G loss: 229.574753] time: 0:30:44.524438\n",
            "[Epoch 0/1] [Batch 127/150] [D loss: 0.296027] [G loss: 566.086914] time: 0:30:58.782204\n",
            "[Epoch 0/1] [Batch 128/150] [D loss: 0.280583] [G loss: 307.166840] time: 0:31:13.091248\n",
            "[Epoch 0/1] [Batch 129/150] [D loss: 0.286461] [G loss: 334.983551] time: 0:31:27.847008\n",
            "[Epoch 0/1] [Batch 130/150] [D loss: 0.290595] [G loss: 492.839325] time: 0:31:42.211529\n",
            "[Epoch 0/1] [Batch 131/150] [D loss: 0.281009] [G loss: 296.673035] time: 0:31:56.553099\n",
            "[Epoch 0/1] [Batch 132/150] [D loss: 0.292099] [G loss: 498.912384] time: 0:32:10.943613\n",
            "[Epoch 0/1] [Batch 133/150] [D loss: 0.294119] [G loss: 426.557343] time: 0:32:25.331729\n",
            "[Epoch 0/1] [Batch 134/150] [D loss: 0.288853] [G loss: 417.557190] time: 0:32:39.752173\n",
            "[Epoch 0/1] [Batch 135/150] [D loss: 0.292948] [G loss: 402.909973] time: 0:32:54.105907\n",
            "[Epoch 0/1] [Batch 136/150] [D loss: 0.281428] [G loss: 478.301544] time: 0:33:08.517483\n",
            "[Epoch 0/1] [Batch 137/150] [D loss: 0.282978] [G loss: 375.436707] time: 0:33:22.958872\n",
            "[Epoch 0/1] [Batch 138/150] [D loss: 0.293045] [G loss: 752.680420] time: 0:33:37.276287\n",
            "[Epoch 0/1] [Batch 139/150] [D loss: 0.280237] [G loss: 245.580078] time: 0:33:51.584204\n",
            "[Epoch 0/1] [Batch 140/150] [D loss: 0.277824] [G loss: 246.294785] time: 0:34:05.849532\n",
            "[Epoch 0/1] [Batch 141/150] [D loss: 0.279022] [G loss: 365.671112] time: 0:34:20.840809\n",
            "[Epoch 0/1] [Batch 142/150] [D loss: 0.273645] [G loss: 328.957336] time: 0:34:34.927357\n",
            "[Epoch 0/1] [Batch 143/150] [D loss: 0.279750] [G loss: 351.852661] time: 0:34:49.049276\n",
            "[Epoch 0/1] [Batch 144/150] [D loss: 0.281144] [G loss: 397.236389] time: 0:35:03.214677\n",
            "[Epoch 0/1] [Batch 145/150] [D loss: 0.278182] [G loss: 450.291809] time: 0:35:17.619120\n",
            "[Epoch 0/1] [Batch 146/150] [D loss: 0.281676] [G loss: 387.105927] time: 0:35:31.913075\n",
            "[Epoch 0/1] [Batch 147/150] [D loss: 0.280331] [G loss: 266.197113] time: 0:35:46.217858\n",
            "[Epoch 0/1] [Batch 148/150] [D loss: 0.281653] [G loss: 485.936707] time: 0:36:00.578971\n",
            "Model saved\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}